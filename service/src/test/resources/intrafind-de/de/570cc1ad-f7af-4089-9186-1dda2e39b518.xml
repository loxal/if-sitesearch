<?xml version='1.0' encoding='UTF-8' ?><documents count="50"><Document><url>https://www.intrafind.de/produkte/elasticsearch/plugins_services/SimFinder_Service</url><id>4DA293F24FC7166500F5CBB7DA66D9EE</id><title>Elasticsearch SimFinder Service</title><language>de</language><body>SimFinder Service Der SimFinder Service gibt zu einem in der Trefferliste ausgewählten Dokument inhaltlich ähnliche Dokumente, die beispielsweise an anderen Speicherorten oder in anderen Systemen abgelegt wurden, zurück. Es kommen dabei  statistische Verfahren zum Einsatz, um die Ähnlichkeit zu berechnen.   Technische Spezifikation: Betriebssysteme Windows Server 2008 R2 oder höher sowie Linux Installationsvoraussetzungen Java 7 Kombinierbare Dienste: Der SimFinder Service kann nur in Kombination mit dem IntraFind Search &amp; Index Service verwendet werden Alle weiteren Services und Plugins können orthogonal kombiniert werden. Dokumentation: SOA-Begleitdokumentation Beispielimplementierung in Java wird bereitgestellt Demo-Verzeichnis mit einfachen Click&amp;Play Nutzungsbeispielen Für wen ist der Service interessant: Unternehmen, die eine Trefferliste um die Suche nach ähnlichen Dokumenten anreichern möchten Embedding Partner Elasticsearch Entwickler Solr Entwickler DEMO-ZUGANG ANFORDERN &gt;&gt;</body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/morphologie</url><id>E97BD02612899742D07E9AAF9E7A3135</id><title>Morphologie</title><language>de</language><body>Morphologie   In der Sprachwissenschaft ist die Morphologie ein Teilgebiet der Grammatik. Die Morphologie befasst sich mit der inneren Struktur von Wörtern und widmet sich der Erforschung der kleinsten bedeutungs- und/oder funktionstragenden Elemente einer Sprache, der Morpheme. Diesbezüglich wird die Morphologie – in Anlehnung an den Terminus „Satzgrammatik“ für die Syntax – auch als „Wortgrammatik“ bezeichnet. (28.11.2012 Wikipedia) zurück  </body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/open-source</url><id>AF2D756E7BE65118B37DFCAFC07D72E6</id><title>Open Source</title><language>de</language><body>Open Source   Als Open Source Software oder Technologie versteht man hauptsächlich die Offenlegung des Quellcodes,Quelltextes. Das gibt Unternehmen die Möglichkeit auf der Basis des Quellcodes einer Software aufzusetzen und eigene Lösungen zu entwickeln oder das Ursprungsprodukt zu modifizieren oder zu verändern. Open Source Communities sorgen in der Regel für eine Weiterentwicklung der Open Source Software, die wiederum als neuer Standard verwendet werden kann. Klassische Open Source Software im Enterprise Serach Umfeld ist Lucene, Solr aber auch ganz neu ElasticSearch. IntraFind setzt auf die Open-Source-Suchmaschine Lucene, baut aber auch Lösungen für alle anderen Komponenten. zurück  </body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/entitaetenerkennung</url><id>A2C234D969D458A5A71795F4D96B579F</id><title>Entitätenerkennung</title><language>de</language><body>Entitätenerkennung   Erkennung von Beschreibungen, Bedeutungen, Wortherkunft oder Übersetzungen. Und um diese Informationen aus einem unstrukturierten Text zu erhalten, nutzt man die Informationsextraktion. Die dabei extrahierten Informationseinheiten, bezeichnet man als Entitäten. Aus linguistischer Sicht stellen solche Entitäten (Informationseinheiten) einen Spezialbereich dar, der für eine bestmögliche Auswertung spezielle Analyseverfahren benötigt. zurück  </body></Document><Document><url>https://www.intrafind.de/produkte/ifinder</url><id>CAE8CED65A6548CB0E304AB0C9703647</id><title>IntraFind Produkte</title><language>de</language><body>INTRAFIND - Ihr Spezialist für das effiziente Suchen &amp; Finden von Informationen und Content Analytics SUCHEN &amp; FINDEN Wichtige Informationen haben Sie mit IntraFind immer parat. IntraFind entwickelt für Unternehmen Produkte und Lösungen, die ein schnelles Suchen und Finden von Unternehmensinformationen ermöglichen. Mit der Integration der Suchlösung iFinder5 elastic gestalten Sie Ihren Informationsalltag neu und erhalten immer die Information, die für Ihre Arbeit in dem Augenblick wichtig ist. Mehr Informationen CONTENT ANALYTICS Digitale Inhalte einfach und schnell verstehen Content Analytics vereint zahlreiche IntraFind Technologien, die es erlauben digitale Inhalte (Big Content, Big Text) aus Webseiten, Dokumenteninhalten aber auch Nutzerverhalten aus Social Media Foren, News Sites, meinungsbildenden Seiten automatisch zu analysieren und zu verstehen. Mehr Informationen</body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/api</url><id>22521C54797D4C6F503CD1905F976775</id><title>API</title><language>de</language><body>API   Application Programming Interface für Programmierschnittstelle in der Informatik um Informationen auszutauschen.  </body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/redundanz</url><id>871864FF22FC1D3A1231830F2897E012</id><title>Redundanz</title><language>de</language><body>Redundanz   Von Redundanz einer bspw. Information spricht man dann, wenn der Inhalt der Information mehrfach vorhanden ist. Diese Informationen sind doppelt, gleichwertig und sind inhaltlich identisch. zurück  </body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/deep-learning</url><id>FB7B0CBE28FA5356A7A500BD0C227AAE</id><title>Deep Learning</title><language>de</language><body>Deep Learning Deep Learning ist ein Teilbereich des maschinellen Lernens und basiert ebenfalls auf einem Set von Algorithmen. Die Datenstruktur stellt hierbei ein künstliches neuronales Netzwerk dar, das eine große Anzahl an Neuronen umfasst, die untereinander Informationen beziehungsweise Datenmengen austauschen. Das System zieht sich hierbei selbstständig Erkenntnisse aus der Analyse von großen Datenbeständen. Praktische Anwendungsbeispiele für Deep Learning sind vollautomatisierte Call Center, selbstfahrende Autos oder Gesichtserkennung bei Fotoprogrammen. Auch NLP – also das Verstehen von natürlichsprachlichen Suchanfragen und die Ausgabe von entsprechenden Ergebnissen – ist ein Anwendungsbeispiel von Deep Learning.        zurück  </body></Document><Document><url>https://www.intrafind.de/impressum</url><id>A4C0C44D13C16210C710391C8783688A</id><title>Impressum</title><language>de</language><body>Impressum   Angaben gemäß § 5 TMG: IntraFind Software AG Landsberger Straße 368 80687 München Deutschland   Vertretungsberechtigter Vorstand: Franz Kögl, Bernhard Messer   Kontakt: Telefon:       49 89 3090446-0 Fax:             49 89 3090446-29 E-Mail:        info@intrafind.de   Registereintrag: Eintragung im Handelsregister. Registergericht:  Amtsgericht München Registernummer: HRB 134378   Umsatzsteuer-ID: Umsatzsteuer-Identifikationsnummer gemäß §27 a Umsatzsteuergesetz: DE 813073059   Quellenangaben für die verwendeten Bilder: Copyright (C) Fotolia 2004-2014: Alliance / bloomua / FotolEdhar   Quelle: eRecht24, Rechtsanwalt für Internetrecht Sören Siebert   Haftungsausschluss:   Haftung für Inhalte Die Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.   Haftung für Links Unser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf mögliche Rechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.   Urheberrecht Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.   Datenschutz Die Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten möglich. Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift oder eMail-Adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger Basis. Diese Daten werden ohne Ihre ausdrückliche Zustimmung nicht an Dritte weitergegeben. Wir weisen darauf hin, dass die Datenübertragung im Internet (z.B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich. Der Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten durch Dritte zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit ausdrücklich widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-Mails, vor.   Datenschutzerklärung für die Nutzung von Facebook-Plugins (Like-Button) Auf unseren Seiten sind Plugins des sozialen Netzwerks Facebook, 1601 South California Avenue, Palo Alto, CA 94304, USA integriert. Die Facebook-Plugins erkennen Sie an dem Facebook-Logo oder dem &quot;Like-Button&quot; (&quot;Gefällt mir&quot;) auf unserer Seite. Eine Übersicht über die Facebook-Plugins finden Sie hier: https://developers.facebook.com/docs/plugins/. Wenn Sie unsere Seiten besuchen, wird über das Plugin eine direkte Verbindung zwischen Ihrem Browser und dem Facebook-Server hergestellt. Facebook erhält dadurch die Information, dass Sie mit Ihrer IP-Adresse unsere Seite besucht haben. Wenn Sie den Facebook &quot;Like-Button&quot; anklicken während Sie in Ihrem Facebook-Account eingeloggt sind, können Sie die Inhalte unserer Seiten auf Ihrem Facebook-Profil verlinken. Dadurch kann Facebook den Besuch unserer Seiten Ihrem Benutzerkonto zuordnen. Wir weisen darauf hin, dass wir als Anbieter der Seiten keine Kenntnis vom Inhalt der übermittelten Daten sowie deren Nutzung durch Facebook erhalten. Weitere Informationen hierzu finden Sie in der Datenschutzerklärung von facebook unter https://de-de.facebook.com/policy.php Wenn Sie nicht wünschen, dass Facebook den Besuch unserer Seiten Ihrem Facebook-Nutzerkonto zuordnen kann, loggen Sie sich bitte aus Ihrem Facebook-Benutzerkonto aus.   Datenschutzerklärung für die Nutzung von Google Analytics Diese Website benutzt Google Analytics, einen Webanalysedienst der Google Inc. (&quot;Google&quot;). Google Analytics verwendet sog. &quot;Cookies&quot;, Textdateien, die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie ermöglichen. Die durch den Cookie erzeugten Informationen über Ihre Benutzung dieser Website werden in der Regel an einen Server von Google in den USA übertragen und dort gespeichert. Im Falle der Aktivierung der IP-Anonymisierung auf dieser Webseite wird Ihre IP-Adresse von Google jedoch innerhalb von Mitgliedstaaten der Europäischen Union oder in anderen Vertragsstaaten des Abkommens über den Europäischen Wirtschaftsraum zuvor gekürzt. Nur in Ausnahmefällen wird die volle IP-Adresse an einen Server von Google in den USA übertragen und dort gekürzt. Im Auftrag des Betreibers dieser Website wird Google diese Informationen benutzen, um Ihre Nutzung der Website auszuwerten, um Reports über die Websiteaktivitäten zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen gegenüber dem Websitebetreiber zu erbringen. Die im Rahmen von Google Analytics von Ihrem Browser übermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengeführt. Sie können die Speicherung der Cookies durch eine entsprechende Einstellung Ihrer Browser-Software verhindern; wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht sämtliche Funktionen dieser Website vollumfänglich werden nutzen können. Sie können darüber hinaus die Erfassung der durch das Cookie erzeugten und auf Ihre Nutzung der Website bezogenen Daten (inkl. Ihrer IP-Adresse) an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter dem folgenden Link verfügbare Browser-Plugin herunterladen und installieren: https://tools.google.com/dlpage/gaoptout?hl=de.   Datenschutzerklärung für die Nutzung von Google  1 Erfassung und Weitergabe von Informationen: Mithilfe der Google  1-Schaltfläche können Sie Informationen weltweit veröffentlichen. über die Google  1-Schaltfläche erhalten Sie und andere Nutzer personalisierte Inhalte von Google und unseren Partnern. Google speichert sowohl die Information, dass Sie für einen Inhalt  1 gegeben haben, als auch Informationen über die Seite, die Sie beim Klicken auf  1 angesehen haben. Ihre  1 können als Hinweise zusammen mit Ihrem Profilnamen und Ihrem Foto in Google-Diensten, wie etwa in Suchergebnissen oder in Ihrem Google-Profil, oder an anderen Stellen auf Websites und Anzeigen im Internet eingeblendet werden. Google zeichnet Informationen über Ihre  1-Aktivitäten auf, um die Google-Dienste für Sie und andere zu verbessern. Um die Google  1-Schaltfläche verwenden zu können, benötigen Sie ein weltweit sichtbares, öffentliches Google-Profil, das zumindest den für das Profil gewählten Namen enthalten muss. Dieser Name wird in allen Google-Diensten verwendet. In manchen Fällen kann dieser Name auch einen anderen Namen ersetzen, den Sie beim Teilen von Inhalten über Ihr Google-Konto verwendet haben. Die Identität Ihres Google-Profils kann Nutzern angezeigt werden, die Ihre E-Mail-Adresse kennen oder über andere identifizierende Informationen von Ihnen verfügen. Verwendung der erfassten Informationen: Neben den oben erläuterten Verwendungszwecken werden die von Ihnen bereitgestellten Informationen gemäß den geltenden Google-Datenschutzbestimmungen genutzt. Google veröffentlicht möglicherweise zusammengefasste Statistiken über die  1-Aktivitäten der Nutzer bzw. gibt diese an Nutzer und Partner weiter, wie etwa Publisher, Inserenten oder verbundene Websites.   Datenschutzerklärung für die Nutzung von Twitter Auf unseren Seiten sind Funktionen des Dienstes Twitter eingebunden. Diese Funktionen werden angeboten durch die Twitter Inc., Twitter, Inc. 1355 Market St, Suite 900, San Francisco, CA 94103, USA. Durch das Benutzen von Twitter und der Funktion &quot;Re-Tweet&quot; werden die von Ihnen besuchten Webseiten mit Ihrem Twitter-Account verknüpft und anderen Nutzern bekannt gegeben. Dabei werden auch Daten an Twitter übertragen. Wir weisen darauf hin, dass wir als Anbieter der Seiten keine Kenntnis vom Inhalt der übermittelten Daten sowie deren Nutzung durch Twitter erhalten. Weitere Informationen hierzu finden Sie in der Datenschutzerklärung von Twitter unter https://twitter.com/privacy. Ihre Datenschutzeinstellungen bei Twitter können Sie in den Konto-Einstellungen unter https://twitter.com/account/settings ändern.   Quellen: eRecht24 Disclaimer, eRecht24 Facebook Datenschutzerklärung, Google Analytics Bedingungen, Datenschutzerklärung Google  1, Datenschutzerklärung Twitter   Letzte Aktualisierung: Juni 2013  </body></Document><Document><url>https://www.intrafind.de/services_support</url><id>ABB4DDED0A233BA9D5C3309F4D97D6C4</id><title>IntraFind Professional Services</title><language>de</language><body>Service &amp; Support Der Geschäftsbereich IntraFind Professional Services umfasst ein umfangreiches Angebot an Beratungs- und Schulungsdienstleistungen, um Kunden mit den verschiedensten Anforderungen bei der Auswahl, der Implementierung und dem Betrieb ihrer Enterprise Search-Lösungen zu unterstützen. Das Team Professional Services besteht aus langjährig erfahrenen Suchexperten, Software-Entwicklern und nach IPMA-Standards zertifizierten Projektmanagern, die über eine umfassende Marktkenntnis verfügen und sich ausschließlich mit dem Thema Suchtechnologien beschäftigen. Im Fokus steht eine ganzheitliche, professionelle und individuelle Betreuung des Kunden, die auf eine langfristige, vertrauensvolle Zusammenarbeit ausgerichtet ist. IntraFind begleitet beispielsweise die Evaluierungsphase von Unternehmen, die die Neueinführung einer Suchtechnologie planen und unterstützt sie mit fundiertem Produkt-Know-how. Hat der Kunde sich für ein Softwareprodukt entschieden oder verfügt bereits über eine Suchlösung, die qualitativ optimiert oder durch ein neues Produkt abgelöst werden soll (z.B. Google Search Appliance - GSA), kann er auf IntraFinds jahrelange Expertise in der Konzeption und erfolgreichen Umsetzung von Enterprise Search-, Content Analytics- und Wissensmanagement-Projekten zurückgreifen. Der spätere Produktivbetrieb und Support der Suchlösung kann von dem Kunden selbst oder von IntraFind übernommen werden. Für den Aufbau von unternehmensinternem Know-how auf Kundenseite bietet IntraFind ein breitgefächertes Angebot an Schulungen zum erfolgreichen Management von Enterprise Search-Projekten, zum IntraFind Enterprise Search-Produkt iFinder sowie zu Methoden und Verfahren der Textanalyse an.</body></Document><Document><url>https://www.intrafind.de/blog/schwierige-verwandtschaft-wie-suchmaschinen-mithilfe-von-wortfamilien-erweitert-werden-koennen</url><id>114E1531F5C1D0193B9E6D292AB7A712</id><title>Schwierige Verwandtschaft – wie Suchmaschinen mithilfe von Wortfamilien erweitert werden können</title><language>de</language><body>Schwierige Verwandtschaft – wie Suchmaschinen mithilfe von Wortfamilien erweitert werden können Über den Mehrwert eines Stem-Thesaurus für optimale Suche und Recherche Was muss eine gute Suchmaschine können? Den Mindestanspruch erfüllen jene Suchfunktionen, die in einem Textverarbeitungsprogramm oder einem Editor üblicherweise mit dem Shortcut STRG   F aufgerufen werden: Man tippt eine Buchstabenfolge ein und das geöffnete Dokument wird auf alle Vorkommen genau dieser Kombination durchsucht. Auf diese Art und Weise lassen sich eindeutig bestimmte Textstellen innerhalb eines Dokuments schnell und einfach finden.   Wörter bestehen allerdings nicht aus immer gleichen Buchstabenfolgen, sondern sind dynamische Gebilde: sie werden je nach grammatischer Person, Zeit, Kasus oder Geschlecht flektiert. Einem Nutzer, der sich umfangreicher über ein bestimmtes Thema oder über die Verwendung eines bestimmten Wortes informieren möchte, wäre eine solche Suchfunktion deshalb sicherlich zu wenig.   Ein Weg zur Erweiterung der Suche ist demzufolge die Lemmatisierung: ein Feature, das neben der Kompositazerlegung den Kern des Linguistik Plugins von IntraFind ausmacht und das auf dem firmeneigenem Vollformenlexikon basiert. So ist z.B. die Buchstabenfolge “dachtest” im Lexikon ihrer Grundform “denken” zugeordnet, ebenso wie alle anderen möglichen Formen von “denken”. Auf diese Weise kann bei einer Suche nach einer dieser Formen die gesamte Bandbreite gebeugter Wortvarianten (Flexionsparadigma) einbezogen werden.   Wörter sind dynamische Gebilde – nicht nur, weil sie flektiert werden können, sondern auch, weil sie durch sogenannte Wortbildungsprozesse zu neuen Wörtern zusammengeführt werden können. Zu diesen Wortneubildungen stehen sie dann in einer Art Verwandtschaftsverhältnis. Diese Verwandtschaftsbeziehung kann zwischen Wörtern aus ein und derselben Wortart bestehen, wie z.B. zwischen den Substantiven “Chemie” und “Chemiker” oder zwischen Wörtern verschiedener Wortarten wie “Chemie” und “chemisch” oder “kaufen” und “Käufer”.   Eine Gruppe von Wörtern, die in einer solchen Beziehung zueinander stehen, nennt man eine Wortfamilie. In vielen Situationen kann es nützlich sein, bei einer Suche nicht nur das Flexionsparadigma, sondern auch die Wortfamilie miteinzuschließen; denn wer nach “Chemie” sucht, für den könnten auch Suchergebnisse interessant sein, die “Chemiker”, “Chemikalie” und “chemisch” enthalten. Zu diesem Zweck entwickelte IntraFind einen Stem-Thesaurus, in dem die Verwandtschaftsrelation zwischen Mitgliedern einer Wortfamilie hinterlegt ist.   Der Thesaurus ist ein Stem-Thesaurus, weil das Kriterium für die Verwandtschaft ein gemeinsamer Wortstamm ist: ein linguistisch umstrittener Begriff, der sich aber in der Anwendung als derjenige Teil des Wortes definieren lässt, der einer Wortfamilie gemeinsam ist und aus dem durch Anhängen von Prä- oder Suffixen die einzelnen Wörter der Wortfamilie entstehen. Der Wortstamm für unsere “Chemie”-Familie wäre also “chem-”, woran dann die Endungen “-isch”, “-ie” und “-iker” gehängt werden können.   Die Untersuchung von Wortbildungsprozessen zeigt, dass es gewisse Regelmäßigkeiten bei der Ableitung von Wörtern aus dem Wortstamm gibt: z.B. entstehen Infinitive fast immer durch ein Anhängen von “-en” oder “-n”, Nomen sehr oft durch das Suffix “-er” oder “-ler”. Die Stämme “spiel-” und “wander-” werden im Infinitiv zu “spiel-en” und “wander-n”, als Nomen zu “Spiel-er” und “Wander-er”.   Regeln dieser Art lassen sich so implementieren, dass ein Teil des Thesaurus automatisch erstellt werden kann. Allerdings wird auch deutlich, dass es in der Sprache zu viele Ausnahmen gibt, als dass man sie einfach mit ein paar zusätzlichen Ausnahmeregeln korrigieren könnte. Während der “Spieler” einfach jemand ist, der spielt, ist die “Leber” nicht eine Person, die lebt.   Besonders schwierig wird die Verwandtschaft bei Vorsilben, welche die Bedeutung des Stammes sehr stark verändern, auch wenn sie durchaus noch mit ihm verwandt sind: “unterschreiben” hat schon etwas mit “schreiben” zu tun, aber es ist doch etwas anderes und sollte bei einer Suchanfrage nicht damit vermischt werden.   Wegen all dieser Ausnahmen ist es sinnvoll, Wortfamilien in einem Stem-Thesaurus zu speichern, der zwar mit automatischer Unterstützung, aber auch mit halbautomatisch erstellten Ausnahmelisten und menschlicher Endkontrolle erstellt wird. Der fertige Thesaurus enthält schließlich nur diejenigen Wortfamilien, innerhalb derer eine hinreichende semantische Verwandtschaft besteht, um sie als zusammengehörig anzusehen. Er steht dann als weitgehend feststehende, aber erweiterbare Ressource zur Verfügung, um das Spektrum von Suchfunktionen um die jeweilige Wortfamilie zu erweitern.   Solche halb-automatisch erzeugten, händisch kuratierten Stem-Thesauri sind wertvolle Ressourcen für die explorative Suche (siehe IntraFind Blogbeitrag &quot;Tagging - Mehrwerte durch Metadaten&quot;). Sie ermöglichen im Zusammenspiel mit den weiteren Features des iFinder5 elastic eine deutliche Erweiterung der Such- und Recherchemöglichkeiten des Benutzers.   Der Autor Pascal Zambito, B.Sc., Absolvent der Computerliguistik an der Ludwig-Maximilians-Universität München (LMU), hat im Rahmen seiner Bachelorarbeit im Auftrag der IntraFind Software AG einen Stem-Thesaurus für die deutsche Sprache entwickelt.</body></Document><Document><url>https://www.intrafind.de/blog/der-ifinder5-elastic-als-content-delivery-portal</url><id>50D5F62605DBD6E9641A2BCCBBAAFAEC</id><title>Der iFinder5 elastic als Content Delivery Portal (CDP)</title><language>de</language><body>Der iFinder5 elastic als Content Delivery Portal (CDP) Ein Produkt - vielfältige Anwendungsmöglichkeiten... Mit der Markteinführung der neuen Produktgeneration iFinder5 elastic ergeben sich immer wieder neue Anwendungsmöglichkeiten. Eine davon ist &quot;Content Delivery&quot;, die auf den Aspekt der anwendungs- und anwendergerechten Bereitstellung relativ statischer und vor allem qualitätsgesicherter Informationen abzielt. Dazu zählen beispielsweise mehrsprachige Bedienungsanleitungen, Schaltpläne, Handbücher - letztlich jede Art von final freigegebenen Informationen, z.B. für die Bedienung und Wartung einer komplexen Anlage oder Maschine. Diese Dokumente und Inhalte werden am Ende eines Content-Management-Prozesses von der iFinder-basierten Content-Delivery-Lösung mit wertvollen Zusatzinformationen angereichert und anwendergerecht bereitgestellt. Stellen Sie sich das Ganze als zweistufigen Prozess vor: Zuerst werden Informationen, meist aus unterschiedlichen Datenquellen, aggregiert und mit Hilfe von Suchtechnologien dargestellt. Der iFinder5 elastic setzt hier im Backend mit Elasticsearch auf die State-of-the-Art NoSQL Indextechnologie und kombiniert diese mit neuesten semantischen und linguistischen Textanalyseverfahren. Der iFinder5 elastic identifiziert alle relevanten Inhalte, der Content Verantwortliche wählt aus und stellt so das für den Anwendungsfall benötige Informationspaket zusammen. Dieses wird anschließend im eigenen Datastore des iFinder abgelegt. Im zweiten Schritt erfolgt die maßgeschneiderte Content-Nutzung - mit IntraFind als Online-Portal, auch responsive auf dem Smartphone oder dem iPad. Der darauf installierte iFinder stellt nun alle Inhalte anwendungs- und anwendergerecht dar. Anwendungsgerecht bedeutet, dass Schaltpläne anders dargestellt werden als komplexe mehrsprachige Handbücher. Anwendergerecht heißt, dass der iFinder den Arbeitsprozess unterschiedlich unterstützt: Der Wissensarbeiter kann über Inhalte und Kontexte navigieren, für ein schnelles Nachschlagen wird eine schlanke Trefferliste mit dem besten Dokument an erster Stelle angezeigt. Ein Techniker kann sich über die Auswahl von Kategorien wie Produkt oder Baugruppe bis zum defekten Einzelbauteil über die Wissenslandkarte navigieren. Jeder Benutzer kann aktiv recherchieren, wird wie in einem modernen Online-Shop geführt und während seiner gesamten Recherche automatisch unterstützt. Dokumente und Metadaten, normalerweise getrennt gepflegt, werden bei der Erfassung der Daten zusammengeführt und dadurch Informationen mühelos in einen Kontext gesetzt. Aus strukturierten Datenquellen wie z.B. PLM-Datenbanken können zum Zeitpunkt der Informationszusammenstellung unstrukturierte Inhalte wie Bedienungsanleitungen mit Metadaten angereichert werden. Ein weiterer Nutzen der Lösung ist die Möglichkeit der automatischen Vernetzung der Inhalte, die einfaches Navigieren ermöglicht. Ein Beispiel soll dies verdeutlichen: Die gesuchte Baugruppe wird als Navigationsobjekt in der Trefferliste angezeigt. Durch Klick auf die Baugruppe werden alle anderen Inhalte auf diese Baugruppe hin gefiltert und der Benutzer sieht zentral und übersichtlich alle Fundstellen - sogar farblich hervorgehoben - im gesamten Dokumentenbestand. Innerhalb eines Dokuments erleichtert eine zusätzliche Übersicht aller Fundstellen die schnelle Erfassung aller Inhalte. Das ist dann &quot;information in context at your fingertips&quot;, fasst IntraFind Vorstand Franz Kögl den Mehrwert der neuen IntraFind Lösung zusammen. Der Autor Manuel Brunner ist Experte für das Thema Suchtechnologien. Seit 2008 ist er für die IntraFind Software AG tätig und leitete u.a. viele Jahre lang das Team Professional Services, bevor er in seiner neuen Rolle die Bereiche Partner Management &amp; Business Development übernahm. Manuel Brunner is an expert in search technologies. He has been working at IntraFind Software AG since 2008 and has managed the team Professional Services for many years. In his new role he took over the divisions Partner Management &amp; Business Development.</body></Document><Document><url>https://www.intrafind.de/blog/language-identification-and-language-chunking-en</url><id>0E080E5C8D42FE6F2DBE6FF25165C977</id><title>Language Identification and Language Chunking</title><language>de</language><body>Language Identification and Language Chunking   Identifying the language of a given text is a crucial preprocessing step for almost all text analysis methods. It is considered as a solved problem since more than 20 years. Available solutions build on the simple observation that for all languages typical letter sequences (letter n-grams) exist, that occur significantly more frequent in this language than in other languages. Even the frequency of individual letters differs significantly for European languages. The letter “e” is the most frequent single letter in most European languages, but for Polish the most frequent letter is “a”.   Figure 1: All letter n-grams of length 1 (unigram) to 4 (tetra gram) for &quot;TEXT&quot; with &quot;_&quot; as marker for start and end of text Tika: Cavnar &amp; Trenkle Approach All existing solutions for language identification build some kind of letter n-gram model or profile for every language, which is then compared with the letter n-gram profile of the text that is analyzed. A very simple approach was described in [C&amp;T94] and is still used by the Apache Tika language identifier. The profiles generated in their approach simply consist of the most frequent letter n-grams sorted according to their frequency and the comparison of text and language profiles is based on a heuristic measure which compares the rank of letter n-grams in both profiles:   Figure 2: Apache Tika / C&amp;T94 Approach   As stated already in the beginning, language identification is considered a solved problem. For text that is longer than 100 characters, there are methods that identify the correct language with an accuracy of 99%. This means that we might still have ten thousand incorrectly classified documents in a 1 million documents collection. So there is definitely still room for improvement. Furthermore, accuracy of established approaches drops considerably if we consider shorter text chunks like tweets or search engine queries.   Since there is an increasing need for accurate language identification for short text, research activity in the field has increased during the last 5 years. At IntraFind we recently also decided to take a closer look at language identification. I think we got some interesting findings.   First we tested the simple Apache Tika implementation on the LIGA benchmark (language identifier benchmark on twitter data, see [T&amp;P11]). We got an accuracy of 97.7%, much better than the 93% reported by [T&amp;P11] with the Tika implementation. In contrast to [T&amp;P11] we used our own training data set which is much bigger than the data set used by [T&amp;P11] (they used cross validation on the benchmark itself). Our result is even slightly better than the result achieved by [T&amp;P11] with their own very sophisticated new approach. This shows that using big sets of training data is important.   Google: Naïve Bayes Established approaches for language identification such as the Naïve Bayes approaches used by the current Google implementations (e.g. the Chromium Language Detector used in the Chrome Browser) use a fixed n-gram length. Usually an n-gram length of 4 is used for European languages and a length of 2 is used for Chinese or Japanese. For a comparison see [McC11].   Latest Developments Latest developments that try to improve language identification accuracy consider longer n-grams. This is motivated by the fact, that languages are not only characterized by typical short letter n-grams. Humans recognize languages because they know frequent words and those words are usually longer than 4 letters.   We tested a language identifier based simply on counting the number of matching words for each language using our morphological lexica and achieved an accuracy of 99.4% on the LIGA benchmark. This is better than the best result reported on that benchmark (as far as we know) and it confirms the idea that a fixed short n-gram length is incompatible with further improvements in language identification. However, language identification is considered as a preprocessing step which should be very fast. Lexicon lookup is not a practical solution, especially since good morphological lexica might not be available for all languages.   Markov Chain Standard Naïve Bayes n-gram approaches do not allow to consider variable-length n-grams in a straightforward way. We therefore decided to use a Markov Chain model [Dun94], which actually allows to use variable-length n-grams by using a back-off approach for estimating letter n-gram probabilities. With our current implementation we achieve an accuracy of 99.2% on the LIGA benchmark, which is identical to the best published result that we know.   Furthermore our Java-based implementation is very fast. On a current notebook processor with one thread we achieve a throughput of 5.5 MB/sec (counting each character as one byte), which is approximately 5 times faster than the Apache Tika implementation and comparable to the performance of the fastest existing language identifier (Google’s Chromium language detector).   Language identification is not the only goal we wanted to achieve with our new language identifier. It can be used to automatically identify chunks with the same language within a given text and since we have a high accuracy for short text we are even able to identify short chunks (see figure 3).   German Tweet: “Ich mag den Song la vie en rose sehr gerne.” Chunk 1: Ich mag den Song language: de Chunk 2: la vie en rose language: fr Chunk 3: sehr gerne language: de Fig 3: Language Chunking Example     Bibliography:   &quot;Gram-Based Text Categorization&quot;, William B. Cavnar , John M. Trenkle, In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, 1994 [T&amp;P11] “Graph-based n-gram language identification on short texts.”, Erik Tromp and Mykola Pechenizkiy, In Proceedings of Benelearn, The Hague, Netherlands, 2011 [Dun94] “Statistical Identification of Language”, Ted Dunning, Technical Report New Mexico State University, 1994 [McC11] “Accuracy and performance of Google&apos;s Compact Language Detector”, Michael McCandless Blog: https://blog.mikemccandless.com/2011/10/accuracy-and-performance-of-googles.html Der Autor Dr. Christoph Goller verfügt über 15 Jahre Erfahrung im Enterprise Search Markt. Er promovierte in Computerwissenschaften an der Technischen Universität in München und arbeitete im Anschluss in einigen wissenschaftlichen Projekten zu Themenbereich Künstliche Intelligenz, Machine Learning sowie Neuronale Netze.   Seit 2002 leitet er den Bereich Forschung bei IntraFind und verantwortet IntraFind&apos;s Kern-Suche- und Content Analyse Technologien. Zwischen 2003 und 2007 arbeitete Christoph aktiv als Committer in Apache&apos;s Open Source Projekt Lucene.    </body></Document><Document><url>https://www.intrafind.de/blog/tagging-mehrwerte-durch-metadaten</url><id>6689BF90707AA359E2137AEED3FCC1C0</id><title>Tagging – Mehrwerte durch Metadaten</title><language>de</language><body>Tagging – Mehrwerte durch Metadaten   Metadaten sind per Definition Daten, welche andere Daten beschreiben. Beispiele hierfür sind Informationen wie Autor, Erstellungsdatum, Ort der Erstellung eines Dokuments oder auch Textbausteine, die besonders relevant sind. Bei IntraFind unterscheiden wir strukturelle Metadaten von semantischen Metadaten - nicht nur weil sie unterschiedlich entstehen, sondern auch weil sie unterschiedlich genutzt werden können.     Strukturelle Metadaten entspringen direkt der Struktur eines Dokuments (oder liegen bereits strukturiert vor). Beispiele für strukturelle Metadaten sind Dateityp oder Erstellungsdatum eines Dokuments.   Semantische Metadaten werden über mehr oder weniger aufwändige Text Analytics-Verfahren aus dem Inhalt eines Dokumentes gewonnen. Beispiele hierfür sind die in einem Text erwähnten Namen von Personen oder Orten oder die in einem Bild erkannten Gesichter. Semantische Metadaten ermöglichen einen inhaltlichen Überblick zu einzelnen Dokumenten ohne umfangreiche Detailbetrachtungen. Darüber hinaus können solche Metadaten im Kontext einer Suche dokumentenübergreifend kombiniert werden und somit einen wichtigen Einblick in die Unternehmensdaten gewähren.   Probleme bei der Erzeugung semantischer Metadaten Semantische Metadaten entstehen dort, wo redaktionelle Arbeit geleistet wird – beispielsweise in der Redaktion einer Zeitung. Üblicherweise sind die Redakteure als Inhaltsproduzenten auch dafür verantwortlich, die Inhalte mit semantischen Metadaten zu versehen; beispielsweise indem sie eine Ressorteinordnung („Politik“, „Sport“, &quot;Wirtschaft&quot;) vornehmen oder besonders relevante Schlagwörter („Eurokrise“, &quot;US-Wahlkampf&quot;) ihres Textes dem Inhalt als sogenannte Tags hinzufügen. Die durch die manuelle und individuelle Verschlagwortung erzeugte Heterogenität erschwert eine dokumentenübergreifende Verwertung der Metadaten (autorenübergreifende Verlinkungen von Texten sind nicht mehr möglich).   Dieses Problem ist in der Bibliothekswissenschaft unter dem englischen Fachbegriff inter-indexer consistency bekannt und der unvermeidlichen Subjektivität der Interpretation des Einzelnen geschuldet. Dies war einer der Gründe, warum unser Kunde ZEIT Online die Unterstützung von IntraFind gesucht hatte und bei der Verschlagwortung von Nachrichten seit Jahren erfolgreich auf unser Produkt Tagging Service setzt. Durch die automatische Verschlagwortung kann eine homogene Schlagwortlandschaft gewährleistet werden. [Pflugfelder und Drongowski 2012]   Eine Meta-Frage stellt sich Ein Anwendungsszenario von Metadaten wurde schon erläutert – Metadaten, besonders semantische, ermöglichen eine automatische Verlinkung von Dokumenten in einem Datenbestand. Die dadurch erzeugten Informationsnetze eignen sich hervorragend als Navigationshilfe und bringen im Fall eines über das Internet frei zugänglichen Informationsbestandes, intelligent konsolidiert, einen enormen Vorteil für dessen Suchmaschinenoptimierung (SEO).   Bei der Verwendung von Metadaten in der Suche sollte zunächst folgende Frage beantwortet werden: Welche Arten/Kategorien von Fragestellungen haben Benutzer einer Suchmaschine und wie können Metadaten sie bei der Abdeckung ihres Informationsbedarfes am besten unterstützen? Dazu ein nicht ganz ernst gemeintes Zitat des ehemaligen Verteidigungsministers der USA, Donald Rumsfeld [Rumsfeld 2012]:   &quot;There are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns – there are things we do not know we don&apos;t know.&quot;   Rhetorisch ist noch viel Luft nach oben, aber rein formal ist das Zitat schlüssig (obwohl unvollständig: eine Kategorie, die der unknown knowns, wurde nicht behandelt).   Um das Potenzial von Metadaten zu illustrieren, konzentrieren wir uns auf zwei Fragenkategorien - die gezielte Suche (known knowns) und die explorative Suche (unknown unknowns).   Die gezielte Suche (known knowns) Einer gezielten Suche liegen Fragestellungen wie diese zugrunde: „Wo ist denn die Präsentation, die Herr Müller letzte Woche geschickt hat, und die auch an Herrn Meyer ging?“.   Die Wissenslandkarte in unserem Enterprise Search Produkt iFinder stellt eine Benutzeroberfläche für gezielte Suchen dar. Dabei wird nicht wie üblich über die manuelle iterative Verfeinerung der Suchanfrage (query) die Treffermenge weiter eingeschränkt, bis man zum erwünschten Treffer gelangt. Stattdessen wird über eine grobe Suchanfrage (im Extremfall die &quot;*-Suche&quot;, welche alle Dokumente eines Datenbestandes als Treffer liefert) zunächst eine große Treffermenge bereitgestellt, die garantiert den gewünschten Treffer beinhaltet. Anschließend wird durch das Anklicken von auf Metadaten basierenden Filterelementen (Facetten) die Treffermenge so lange eingeschränkt, bis der gewünschte Treffer gefunden wird.   Im obigen Beispiel würde der Benutzer folgendermaßen vorgehen:   der Suchfilter &quot;Dateityp&quot; wird auf den Wert &quot;Präsentation&quot; begrenzt, der Suchfilter &quot;Absender&quot; wird auf den Wert &quot;Herr Müller&quot; festgelegt, der Suchfilter &quot;Änderungsdatum&quot; wird auf den Zeitrahmen &quot;letzte Woche&quot; eingeschränkt und der Suchfilter &quot;CC-Adressat&quot; wird auf den Wert &quot;Herr Meyer&quot; limitiert. Bei jedem Schritt verändert sich die gezeigte Treffermenge und sobald das gesuchte Dokument erscheint, kann der Recherchevorgang erfolgreich beendet werden.   Die gezielte Suche bringt man als Benutzer nicht zwangsläufig in Verbindung mit einer Suchmaschine – schließlich ist diese Art von Suche bei Websuchmaschinen nur bedingt durchführbar. Google und Co. sammeln für einzelne Webseiten nur sehr wenige vom Benutzer sichtbare und verwertbare Metadaten. Außerdem sind solche Suchmaschinen nicht vollständig – sie indizieren nicht das gesamte Internet. Die geschilderten Suchaufträge stehen in der Regel dann an, wenn man in den Daten des eigenen Rechners oder im Dateisystem der Firma ein bestimmtes Dokument sucht. Dabei ist man auf die eigene Organisationsdisziplin und die der Kollegen angewiesen. Solche Suchen beanspruchen viel Zeit und enden oft ohne Erfolg. Im iFinder lässt sich dieses Anwendungsbeispiel als Teil der Suche abdecken.   Die explorative Suche (unknown unknowns) Die explorative Suche ist eine sehr aufwändige Art von Suche. Der Informationsbedarf ist dabei nicht einfach zu verbalisieren oder lässt sich durch das Finden eines einzelnen Dokumentes nicht abdecken. Beispiele für Fragestellungen, welche zu einer explorativen Suche leiten würden, sind: „Worum ging es denn im Projekt XYZ?“, „Was hat Herr Müller in dieser Firma gemacht?“, „Welche sozialen Netzwerke zwischen Personen und Firmen lassen sich aus einem beschlagnahmten Datenbestand ableiten?“.   Die Antworten zu derartigen Fragen stehen selten konsolidiert in einem Dokument zur Verfügung, sondern müssen aus unterschiedlichen Dokumenten und Quellen mühsam zusammengetragen werden. Durch die Kombination von semantischen Metadaten und Suche kann sich ein Benutzer sehr schnell einen Überblick zu solchen Fragestellungen verschaffen. Eine Möglichkeit ist die Bereitstellung von Tag Clouds, die dem Benutzer auf einen Blick besonders relevante Begriffe zur Treffermenge seiner Suchanfrage liefern.   Explorative Suchen fallen beispielsweise bei der Einarbeitung in neue Themen, der Einarbeitung neuer Kollegen oder im Rahmen von Ermittlungen bzw. im Anwendungsbeispiel E-Discovery an. Semantische Metadaten sind eine Voraussetzung für hochwertige Suchergebnisse im Rahmen einer explorativen Suche und durch den Wegfall sonst sehr aufwändiger Recherchearbeiten ein Garant für einen schnellen ROI!   Zusammenfassung Durch den Einsatz des IntraFind Tagging Service lassen sich zusätzlich zu den strukturellen Metadaten automatisch hochwertige semantische Metadaten ohne subjektiven Bias erzeugen, den einzelnen Dokumenten zuordnen und als Filterelemente in der Suche verwenden. Über die Wissenslandkarten im iFinder kann der Benutzer sowohl strukturelle als auch semantische Metadaten verwenden, um gezielte Suchbedürfnisse zu befriedigen.   Durch die Gewinnung semantischer Metadaten bereitet der Tagging Service den Weg zur explorativen Suche. Metadaten gewähren einen detaillierteren Einblick in einen Informationsbestand, optimieren Rechercheprozesse und schaffen kürzere und unkompliziertere Wege zur gewünschten Information.     Bibliographie: [Pflugfelder und Drongowski 2012] – Bernhard Pflugfelder und Ron Drongowski, Semantische Suche @ ZEIT Online, KnowTech -- 14. Kongress zum Wissensmanagement in Unternehmen und Organisationen &quot;Neue Horizonte für das Unternehmenswissen -- Social Media, Collaboration, Mobility&quot;. M. Bentele, N. Gronau, P. Schütt, M. Weber (Hrsg.). Stuttgart, 2012 [Rumsfeld 2012] - Donald Henry Rumsfeld in einem News Briefing des US-Verteidigungsministeriums am 12. Februar 2002 -- https://de.wikiquote.org/wiki/Donald_Rumsfeld (Abruf am 22.07.2015) Der Autor Breno Faria, Head of Development, ist seit 2012 für die IntraFind Software AG tätig. Seit den späten 2000er Jahren beschäftigt er sich intensiv mit den Themen Content Analytics und Information Retrieval. 2015 übernahm er die Rolle des Entwicklungsleiters bei IntraFind.   Im Rahmen von Veranstaltungen, z.B. Berlin Buzzwords 2014 oder &quot;IntraFind Enterprise Search Day 2015&quot;, referiert er regelmäßig über neue Technologien oder präsentiert innovative Lösungen aus IntraFind Kundenprojekten.</body></Document><Document><url>https://www.intrafind.de/blog/the-difference-between-stemming-and-lemmatization-en</url><id>970CE4EC31A4B97B528D153411B896BB</id><title>The difference between stemming and lemmatization</title><language>de</language><body>The difference between stemming and lemmatization   &quot;Stemming&quot; as well as &quot;Lemmatization&quot; are commonly used buzzwords in the field of Information Retrieval (IR), particularly in the development of powerful search engines.   The inverted index Just as a quick reminder: The basis of a search engine is an index, called Inverted Index, a data structure which consists of a list of all unique words (index terms) occurring in any document of a document silo. Additionally, for each unique term a list of documents, in which the term appears, is saved.   As an example, let&apos;s assume that we have to index two documents: First document (doc1): In my house lives a mouse. Second document (doc2): Mice are living in my houses.   The (lowercased) inverted index will look like this: index term documents in doc1, doc2 my doc1, doc2 house doc1 lives doc1 a doc1 mouse doc1 mice doc2 are doc2 living doc2 houses doc2   Now imagine the search query &quot;mouse&quot; (on the above index). It will end up only in the first document as a search result, although document two is also an expected result candidate, as it contains &quot;mice&quot;, the plural of mouse.   A search engine of high quality must be able to handle those linguistical variations of index and search terms. Consequently, some kind of term normalization is indispensable. Stemming and lemmatization are both natural language processing techniques to make sure that different word variants (inflectional and derivational word forms) are not left out.   So what exactly is the difference between these two methods? What are the advantages and disadvantages and which one should be preferred?   Stemming vs. lemmatization   Stemming is a procedure to strip inflectional and derivational suffixes from index and search terms with the aim to merge different word forms into one canonical form, called stem or root. The most common stemmer is the Porter Stemmer (a Porter stemmer implementation is also provided by Lucene library), which works by heuristically (rule based) identifying word suffixes and then chopping them off.   By contrast, lemmatization means reducing an inflectional or derivationally related word form to its baseform (dictionary form) by applying a lookup in a word lexicon. More exactly, the mentioned word lexicon is a dictionary which covers a complete morphological analysis for each word of a specific language.   Advantages of a stemmer are that there are freely available implementations and that there is no need of lexicons, which have to be maintained. However, the quality of stemming often is bad.   Just have a look at some examples produced by Porter Stemmer: Whereas for example &quot;organization&quot; as well as &quot;organs&quot; are both stemmed to &quot;organ&quot; (over-stemming - stemmer is cutting off too much with the result that words of different meaning are reduced to the same root), the following two terms of same origin &quot;absorption&quot; and &quot;absorbing&quot; are stemmed to &quot;absorpt&quot; and &quot;absorb&quot; (under-stemming). There are many other examples where stemming algorithms fail, especially words of irregular inflection (foot [singl.] - feet [pl.], go [inf.] - went [past tense], ...). The problem concerning a search index is obvious - query &quot;organization&quot; will correctly match documents containing &quot;organization&quot; or &quot;organizations&quot;, but will also erroneously merge documents containing &quot;organs&quot;.   By contrast, if an efficient and sufficiently complete lexicon exists, a lemmatizer will mostly output correct baseforms. Thus, a search engine based on a lemmatization normalization component compared to a stemming component significantly benefits and provides much more accurate search results.   High recall and precision for enterprise search   In order to guarantee high recall and precision of iFinder&apos;s search results, IntraFind developed its own linguistic module called LISA, including a lemmatizer of high quality. LISA offers a complete morphological analysis by using complex prepared dictionaries of currently 15 European languages (Croatian, Czech, Dutch, English, French, German, Greek, Hungarian, Italian, Polish, Portuguese, Russian, Slovakian, Slovenian, Spanish).   Apart from lemmatization LISA is also a word decomposer, which is capable of splitting compound words in their individual word fragments. Imagine for example an English search query like &quot;toothpaste&quot;. Thanks to LISA you will be able to find documents, which contain of course &quot;toothpaste&quot;, as well as &quot;paste&quot; and &quot;tooth&quot;.   As German is a language much more complex than English, the following example is more exciting: The German search query &quot;Glückskeks&quot; decomposed by LISA results in two word parts - &quot;glück&quot; and &quot;keks&quot;. As you can see, LISA even succeeds in recognizing the semantic useless part of the word (&quot;s&quot;), called epenthesis (in German &quot;Fugenelement&quot;) and therefore the query will provide result documents containing &quot;glückskeks&quot;, &quot;glück&quot; and &quot;keks&quot;.   For enterprise search, using high performance linguistics like LISA means that the user gets highly relevant search results and no information is lost.   Der Autor Ursula Seisenberger studierte Computerlinguistik an der Universität in München (Center for Information and Language Processing).   Seit 2013 arbeitet Ursula als Software Architektin bei IntraFind und fokussiert sich auf Textanalyseverfahren.</body></Document><Document><url>https://www.intrafind.de/produkte/elasticsearch/plugins_services/search_index_service</url><id>4563227EE9CC98C2B4B4883B4FC789ED</id><title>Elasticsearch Search&amp;Index Service</title><language>de</language><body>Search &amp; Index Service Der Index Service erleichtert durch einfach gestaltete Schnittstellen eine schnelle Indizierung großer Datenmengen. Dieser Indizierungsvorgang wurde bereits mit mehr als 200 Mio. Dokumenten auf Basis von Elasticsearch erfolgreich getestet. Der Search Service bietet einfachen Zugang zu den indizierten Daten und bleibt bei einer Suche in großen Datenmengen stets performant. Interessant ist die Abstraktion beider Services von der tatsächlich verwendeten Indizierungstechnologie. Das bietet eine sehr gute Updatefähigkeit oder einen Wechsel der Suchtechnologie ohne Auswirkungen auf die erstellte Benutzeroberfläche in Kauf nehmen zu müssen.   Technische Spezifikation: Betriebssysteme Windows Server 2008R2 oder höher sowie Linux Installationsvoraussetzung JAVA 7 und Elasticsearch - Bestandteil des Installationspakets Kombinierbare Dienste: Der Index Service benötigt in der Regel einen vorgeschalteten Extract Service, um die Textinformation aus den Dokumenten zu extrahieren Alle weiteren Services und Plugins können in einer Pipeline orthogonal kombiniert werden Dokumentation: SOA-Begleitdokumentation Beispielimplementierung in Java wird bereitgestellt Demo-Verzeichnis mit einfachen Click&amp;Play-Nutzungsbeispielen Für wen ist der Service interessant: Unternehmen, die mit flacher Lernkurve einen Einstieg in State-of-the-Art Indizierungs-Technologien wünschen und höchste Anforderungen an Ausfallsicherheit und Lastverteilung durch horizontale Skalierung haben Unternehmen mit Informationsdatenbanken, die einen höchst performanten Zugriff auf die Information benötigen Unternehmen, die in eine zukunftssichere Lösung investieren wollen Embedding Partner Elasticsearch Entwickler Solr Entwickler EVALUIERUNGSLIZENZ ANFORDERN &gt;&gt;</body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/tag-cloud</url><id>E58FCFD4A7C9D237DDCC7B4B34105A05</id><title>Tag Cloud</title><language>de</language><body>Tag Cloud   Summierung von Merkmalsausprägungen, die aufgrund ihrer Größe von Schrift und Form eine Gewichtung wiedergeben. zurück  </body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/tagging-service</url><id>68431C7E4D622D8054650CB19E466294</id><title>Tagging Service</title><language>de</language><body>Tagging Service   Der IntraFind Tagging Service ist ein Produkt zur automatischen Erzeugung von Metadaten aus Dokumenten. Diese können anschließend für weiterführende Prozesse genutzt werden, z.B. für die unternehmensweite Suche. zurück  </body></Document><Document><url>https://www.intrafind.de/produkte/ifinder5-elastic/nutzenargumente-fuer-unternehmensbereiche</url><id>E8D6B3D784D20F1755775F2FF2BE807B</id><title>iFinder5 elastic Zielgruppen</title><language>de</language><body>Profitieren kann jeder von unserer Suche. Vom einzelnen Anwender bis hin zu ganzen Teams IntraFind beschäftigt sich seit dem Jahr 2000 damit, wie Menschen ihren Arbeitsalltag gestalten. Im Vordergrund liegen dabei alle Prozesse der Informationsbeschaffung. Welche Tools nutzen Mitarbeiter für Ihre Suche nach wichtigen Dokumenten und wie gehen sie dabei vor. Das Ergebnis ist unsere Suchmaschine speziell für Unternehmenszwecke entwickelt, die viele Vorgehens- und Denkweisen der Informationssuche einbezieht. In unseren Beispielen erläutern wir Ihnen einige wichtige Gruppierungen, Abteilungen innerhalb eines klassischen Unternehmens. Abteilungsübergrei­fende Nutzenvorteile Von der einfachen Suche bis hin zum Informationscenter. Abteilungen profitieren von der Auflösung von Informationssilos und kollaborativen Instrumenten... Lesen Sie mehr Fachabteilungen - generisch Suche über alle Laufwerke / File Systeme hinweg erlaubt einen ganzheitlichen Blick in die Inhalte bereits erstellter Dokumente... Erfahren Sie mehr Research &amp; Development Recherchearbeiten, Zusammenhänge erkennen, Wichtiges nicht übersehen - mit dem iFinder5 elastic haben Sie die ideale Unterstützung für Ihre Arbeit. Weiterlesen Kundensupport &amp; Customer Service Erleichtern Sie Ihren Kunden und Mitarbeitern den Zugang zu Service-Informationen, Lösungsansätzen oder FAQ-Hilfen. Optimieren Sie die Produktivität Ihres Kundenservice, Helpdesk oder After Sales Service. Jetzt optimieren Web Development &amp; Portale Sie verantworten für Ihr Unternehmen Intranet, Extranet sowie Portalanwendungen? Vielleicht betreuen Sie auch die Unternehmenswebseite? Dann lesen Sie, wie der iFinder5 elastic Ihre Leser unterstützt. Weiterführende Informationen IT-Abteilungen IntraFind Experten unterstützen und begleiten Suchprojekte nachhaltig - von der Konzeptphase bis zum Go-Live. Erfahren Sie mehr</body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/Crawler</url><id>DEC089BE34324C06E374E2549B11340F</id><title>Crawler</title><language>de</language><body>Crawler   Die Indexing Engine oder der Crawler beschafft die Daten und Dokumente aus den Datenquellen und führt sie anschließend  in einer Struktur entsprechend auf, die wiederum die Suche vereinfacht. Gleichzeitig erstellt die Crawling Engine Caches der Dokumente, damit eine Dokumentvorschau dargestellt werden kann. Dieser Index wird von der Query Engine durchlaufen, um die Treffer aufzulisten.  </body></Document><Document><url>https://www.intrafind.de/blog</url><id>BA441E1FCAE926D5999C6F90C77F3CA9</id><title>Enterprise Search Blog</title><language>de</language><body>ENTERPRISE SEARCH BLOG Auf dieser Seite finden Sie aktuelle Blogbeiträge der IntraFind Software AG. Erfahren Sie von unseren Experten Wissenswertes zu den Themen Enterprise Search, Tagging und Content Analytics.</body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/e-commerce</url><id>EB6DB7984002AFACFAD6B3199128625F</id><title>E-Commerce</title><language>de</language><body>E-Commerce   E-Commerce = elektronischer Geschäftsverkehr, elektronischer Handel.   Geschäftsprozesse wie der Verkauf von Waren oder Dienstleistungen werden zunehmend automatisiert über elektronische Handelsplattformen (z.B. Online Shops oder Portale) im Internet abgewickelt. Eine leistungsfähige Suche, d.h. die Möglichkeit für Portalbesucher schnell und einfach die gewünschten Artikel und Inhalte auf dem Portal zu finden, kann sich entscheidend auf deren Verweildauer bzw. auf generierte Umsätze auswirken.   IntraFind bietet für Betreiber von E-Commerce-Plattformen umfassende Unterstützung bei der qualitativen Optimierung der Shop-Suche wie verbesserte Sprachfeatures durch den Einsatz von Textanalyse, geführte Suche und Auswertungsmöglichkeiten durch die Verwendung der Wissenslandkarte oder Hilfe bei der Datenerfassung und -veredelung. Mehr über die Branchenlösung E-Commerce zurück  </body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/tag</url><id>9DC11F4C8DD2C7CE298CF95A70ECC9EF</id><title>Tag</title><language>de</language><body>Tag   Ein Merkmal, das einen Inhalt, Bild, Text beschreibt. zurück  </body></Document><Document><url>https://www.intrafind.de/blog/10-jahre-automatische-textklassifikation-mit-topicfinder-ein-erfahrungsbericht-aus-der-praxis-374</url><id>BAA7DB058FD1357DA5D880C742D348F9</id><title>10 Jahre automatische Textklassifikation mit TopicFinder. Ein Erfahrungsbericht aus der Praxis.</title><language>de</language><body>10 Jahre automatische Textklassifikation mit TopicFinder. Ein Erfahrungsbericht aus der Praxis.   Kürzlich stellte ich fest, dass 10 Jahre vergangen sind seit dem ersten Release des TopicFinders, unseres Produkts für die automatische Textklassifikation. Seitdem führten wir mit dem TopicFinder zahlreiche Kundenprojekte durch und die Weiterentwicklung des Produkts wurde maßgeblich durch die Praxiserfahrung getrieben. Ich denke, wir können uns durchaus zu Recht als Pioniere der Anwendung automatischer Textklassifikation bezeichnen. Das Jubiläum nehme ich zum Anlass für einen kleinen Blog-Beitrag.   Zunächst zur Begriffsklärung: Unter automatischer Textklassifikation versteht man die automatische Zuordnung von Dokumenten zu vordefinierten Themen auf Basis des Dokumenteninhalts. Ziel der Automatisierung ist fast nie, menschliche Experten zu ersetzen. Eher geht es darum, die großen Datenmengen, die durch menschliche Experten nicht mehr handhabbar sind, durch automatische Filterung in den Griff zu bekommen.   Das Anwendungsspektrum reicht von fachlich anspruchsvoller Klassifikation von juristischen Dokumenten (z.B. Gerichtsurteilen) oder Patenten (Internationale Patentklassifikation nach IPC oder ECLA) über die Filterung von Nachrichtentexten bis hin zur Produktklassifikation (z.B. nach Produktkatalogen ECLASS oder UNSPSC) oder zur Formularerkennung.   Grundlage für die Zuordnung von Dokumenten zu Themen bilden Klassifikationsregeln, die meist ein sehr ähnliches Format wie Queries in der Volltextsuche haben. Jedoch sind die Klassifikationsregeln für jedes Thema fest hinterlegt und werden eher selten geändert. Außerdem sind sie in der Regel sehr viel komplexer als normale Queries, um das jeweilige Thema möglichst umfassend abzudecken und trotzdem scharf genug von anderen Themen abzugrenzen.   Klassifikationsregeln manuell zu erstellen und zu pflegen ist sehr aufwändig. Deshalb setzten wir von Anfang an auf statistische Verfahren, die Klassifikationsregeln automatisch auf Basis von Beispieldokumenten (Trainingsdokumenten) für die jeweiligen Themen generieren. Statistische Verfahren sind objektiver als menschliche Experten, wodurch auch die Klassifikationsqualität steigt. Außerdem können die mit statistischen Verfahren erzeugten Regeln sehr viel komplexer werden, als von Menschen erstellte Regeln. Sie umfassen oft mehrere hundert Einzel- und Mehrwortbegriffe.     Was zeichnet nun den TopicFinder gegenüber anderen Produkten für die automatische Textklassifikation aus? Was haben wir aus der Praxis gelernt?   Der TopicFinder arbeitet nicht ausschließlich auf statistischer Basis. Als Vorverarbeitungsschritt setzen wir die aus unseren Enterprise Search-Produkten bekannten morphologischen Analyzer zur Normalisierung von Wörtern ein und wir nutzen Wortkategorien zur Erzeugung von Mehrwortbegriffen (Nominalphrasen). Durch diese Vorverarbeitung versorgen wir die nachfolgenden statistischen Verfahren mit zusätzlichem Wissen. Die Klassifikationsqualität steigt bzw. es werden weniger Beispieldokumente zum Lernen benötigt. Im TopicFinder verwenden wir Support Vektor Maschinen, die besten verfügbaren statistischen Verfahren. Durch Kreuzvalidierungsläufe wird vollautomatisch die Modellkomplexität optimiert. Gemäß Occams Rasiermesser generieren wir möglichst einfache Modell- / Klassifikationsregeln. Die Erfahrung zeigt, dass man der manuellen Klassifikation durch Experten nicht blind vertrauen darf. Die Beispieldaten, die man vom Kunden zum Training erhält, enthalten in den meisten Fällen auch falsche Themenzuordnungen. Sehr oft werden passende Themenzuordnungen für Beispieldokumente übersehen. Selbst wenn viel Wert auf eine hochwertige, konsistente Klassifikation gelegt wurde und die Beispieldokumente aus seit Jahren gepflegten Taxonomien stammen, so ist doch mindestens mit einer Fehlerrate von 5% zu rechnen. Der TopicFinder kann automatisch inkonsistente Themenzuordnungen in Beispieldaten entdecken, z.B. wenn sehr ähnliche Dokumente unterschiedliche Themenzuordnungen aufweisen. Auch den automatisch erzeugten Klassifikationsregeln darf man nicht blind vertrauen. Der TopicFinder bietet Unterstützung beim Testen der automatisch generierten Regeln, z.B. durch automatische Kreuzvalidierung. Ein Alleinstellungsmerkmal ist, dass die automatisch generierten Klassifikationsregeln lesbar sind und somit ihre Plausibilität durch Experten überprüft werden kann. Klassifikation mit dem TopicFinder ist also keine Blackbox. Mittels farbigem Highlighting machen wir auch für einzelne Dokumente sichtbar, warum sie einem Thema zugeordnet werden. TopicFinder next generation - wohin geht die Reise?   Dieses Jahr werden wir die Administrations- und Trainingsoberfläche des TopicFinder komplett überarbeiten. Der Import von Trainingsdaten wird flexibler durch Skripte steuerbar werden. Den Aufwand für die Erzeugung von manuell klassifizierten Trainingsdaten wollen wir durch die Einführung einer Active Learning-Komponente deutlich reduzieren und wir wollen mehr Unterstützung für Continuous Learning (automatische Verbesserung der Klassifikation durch Nutzung von User-Feedback) bieten.   Der Autor Dr. Christoph Goller verfügt über 15 Jahre Erfahrung im Enterprise Search Markt. Er promovierte in Computerwissenschaften an der Technischen Universität in München und arbeitete im Anschluss in einigen wissenschaftlichen Projekten zu Themenbereich Künstliche Intelligenz, Machine Learning sowie Neuronale Netze.   Seit 2002 leitet er den Bereich Forschung bei IntraFind und verantwortet IntraFind&apos;s Kern-Suche- und Content Analyse Technologien. Zwischen 2003 und 2007 arbeitete Christoph aktiv als Committer in Apache&apos;s Open Source Projekt Lucene.    </body></Document><Document><url>https://www.intrafind.de/blog/counting-counts-arguments-for-using-statistics-to-process-language-en</url><id>34380822588A313E4E07748B90498261</id><title>Counting counts – arguments for using statistics to process language</title><language>de</language><body>Counting counts – arguments for using statistics to process language In my early student years I had a casual conversation with a computer science researcher. The researcher worked on home care robotics for the elderly. I got interested and wanted to know more about it. The objective was to have a robot learn how to cook, clean a toilet and wash the dishes. All by itself. And: &quot;at some point a machine will be able to understand text and learn stuff from the internet, so we concentrate on what happens after that&quot;. This struck me as a very bold assumption. Here is an analogy: thinking about what kind of special stretching exercises I should do once I beat Usain Bolt on the 100m sprint. I&apos;m not arguing that it&apos;s impossible (you don&apos;t know me!), and I won&apos;t deny the possibility of language understanding becoming a fact, but it was the hardest part in that system. And it got my attention.   Very soon I got my hands on a book on precisely this topic called &quot;Foundations of Statistical Natural Language Processing&quot; by Christopher Manning and Hinrich Schütze. It is a remarkably well written book, by authors that don&apos;t shy away from looking beyond the border of their field. The introductory chapter puts the approach of a quantitative, non-symbolic and non-logical Natural Language Processing (the authors&apos; definition of statistical NLP) into its historic and language-philosophical context.    In this post I want to revisit some aspects of this introduction and go a little deeper into what is only one short section on page 17, namely Ludwig Wittgenstein&apos;s argument of &quot;meaning is use&quot; (Philosophical Investigations, 1953), and how it can be seen as a philosophical justification for statistical NLP, including machine learning. Empirical arguments – the statistical modelling of language works The successful use of statistical models in language is naturally a killer argument for using these methods. Naive Bayes classifiers, Latent Dirichlet Allocation models, Support Vector Machines, Deep Neural Nets, Hidden Markov Models, have all achieved remarkable results on several NLP tasks in the last years. But how did this all begin?   The American linguist George K. Zipf was one of the first (early 20th century) to study statistical properties of language. He found out, that if one orders the words of a language according to their frequency – giving the most frequent word the rank 1, the second rank 2, etc. – the product of frequency * rank will remain approximately constant for all words. In other words, the most common word occurs approximately twice as often as the second most common word, and three times as often as the third most frequent word, and so on. This remarkable property of natural languages is called Zipf&apos;s Law. Several well-known best practices in statistical NLP go back to this discovery, e.g. ignoring words in stopword lists. These are lists of the most common words in a corpus that occur that often that they don&apos;t carry any statistical importance for a particular task. Zipf also discovered that there is a negative correlation between a word&apos;s length and its frequency as well as a positive correlation between age and frequency.   These simple, yet remarkable findings show that there is something intrinsically statistical in natural languages. It&apos;s not clear what explains these phenomena – maybe the principle of least effort that Zipf believes is a key characteristic of humans; maybe it&apos;s a greater mathematical law governing many things including language. But we shouldn&apos;t look for causality; it suffices to accept that these properties are measurable.   1948 an engineering breakthrough happened in the area of communication – the development of Information Theory by Claude Shannon. Shannon worked at the famous Bell Labs on communication systems and devised a mathematical theory of communication on noisy channels. He showed that regardless of the noise in a communication channel, it is possible to communicate error-free through it. This works up to a maximum bandwidth, defined by the actual bandwidth and the noise. Information Theory provides the pillars for all modern communication systems and models communication as a stochastic process (a statistical model). Key concepts in Information Theory as &quot;Entropy&quot; or &quot;Mutual Information&quot; have been found to relate closely to and be useful in the processing of natural languages. Information Theory provides, for example, the theoretical lower limit achievable by compressing a text in a particular language. The language-philosophical argument – Wittgenstein and “meaning is use” All these empirical proofs showing that the statistical modeling of language is actually not a farfetched idea give us more or less quantitative assurance that these methods have their entitlement to exist. Wittgenstein provides us with what could be called a qualitative argument that is based on how he understands the inner workings of natural languages.   Wittgenstein argues that for a very large class of words there are no strictly definable meanings, in a mathematical sense. The question &quot;what is the meaning of areté?&quot;, for example, is a question one cannot answer in a satisfying way (ask Meno! [https://en.wikipedia.org/wiki/Meno]). This argument seems strange at first, since we&apos;re used to asking precisely this question – even more so when learning a new language. Wittgenstein claims that the meaning of a word is given by its use in the language.      From §43: &quot;Man kann für eine große Klasse von Fällen der Benützung des Wortes &quot;Bedeutung&quot; – wenn auch nicht für alle Fälle seiner Benützung – dieses Wort so erklären: Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache.&quot;   Take as an example the German word &quot;Spiel&quot; (§66-§71), which is a noun very close to the English &quot;game&quot;, but is used to describe also leisure activities small children do, as playing with a ball, or playing catch, etc. If one is asked what the meaning of &quot;Spiel&quot; is, one idea could be to try and find one single common feature permeating all kinds of &quot;Spiel&quot;. But what would the single common feature be between a professional game of chess and the kicking of a ball against a garage door? Are they all entertaining? Think about the game of Russian roulette! Oh, maybe all &quot;Spiele&quot; have a winner and a loser? Who wins and who loses in a game of solitaire? Maybe it&apos;s all about agility? But compare the agility necessary in chess with the one needed in soccer; and what agility do you need for a game of Bingo? One will quickly realize that all &quot;Spiele&quot; are part of an incredibly complex web of relations, but I defy anyone to find one single common character shared by all &quot;Spiele&quot;.   Wittgenstein calls this &quot;Familienähnlichkeiten&quot; (family resemblances). As members of a family share several characteristics, e.g. expressions, height, eye color, or temper, but never all of them at once, words in a family also share several features. Another very nice analogy is that of a thread. The thread is composed of thousands of fibers that interleave, some touching each other, most not touching each other at all, being very distant from another – but together composing one single object we call a thread. &quot;Spiel&quot; is one such thread, composed of many fibers (instances of a &quot;Spiel&quot;), some sharing portions of what makes them a &quot;Spiel&quot;, some not.   The only way to explain the &quot;meaning&quot; of &quot;Spiel&quot; is to actually enumerate all known uses of &quot;Spiel&quot; in the language. One could say that &quot;Spiel&quot; has a fuzzy definition – but does this make the word useless? I believe not. I even do believe that this fuzziness is more informative and meaningful than if there was a clear definition of &quot;Spiel&quot;.   Of course, this exercise can be expanded to several other words. The first time I read these aphorisms, they resonated with my experiences and I was very quickly very much convinced by them (&quot;Yes! This is how I understand language!&quot;). The beauty of this idea is that it provides a very convincing philosophical theory that justifies the approach of collecting examples of text (corpora) and trying to learn patterns from them, which is the way statistical NLP and machine learning work. Furthermore, Wittgenstein&apos;s argument of &quot;meaning is use&quot; implies that there is no other way to understand a large part of language; this is one fundamental nature of language, and this is how humans understand and use it.   And this is a damned good argument for doing our job the way we do it.   Der Autor Breno Faria, Head of Development, ist seit 2012 für die IntraFind Software AG tätig. Seit den späten 2000er Jahren beschäftigt er sich intensiv mit den Themen Content Analytics und Information Retrieval. 2015 übernahm er die Rolle des Entwicklungsleiters bei IntraFind.   Im Rahmen von Veranstaltungen, z.B. Berlin Buzzwords 2014 oder &quot;IntraFind Enterprise Search Day 2015&quot;, referiert er regelmäßig über neue Technologien oder präsentiert innovative Lösungen aus IntraFind Kundenprojekten.</body></Document><Document><url>https://www.intrafind.de/blog/suche-in-internet-extranet-und-intranet-drei-seiten-einer-medaille</url><id>8F8FF07D55002428979140666E6167F9</id><title>Suche in Internet, Extranet und Intranet. Drei Seiten einer Medaille.</title><language>de</language><body>Suche in Internet, Extranet und Intranet. Drei Seiten einer Medaille.   Viele Unternehmen stehen bei der Einführung einer Suche vor der Herausforderung, verschiedene Bereiche in einer Suche zu vereinheitlichen: sowohl in organisatorischer (Standorte, Abteilungen, usw.) als auch in technischer Hinsicht (Datenquellen, Zugriffsrechte, etc.).   Daraus ergeben sich spezielle Anforderungen, die ich im Folgenden exemplarisch anhand der Suche in Internet, Extranet und Intranet etwas näher beleuchten werde. DATENQUELLEN Ohne Daten keine Suche: Die zu durchsuchenden Daten bilden die Grundlage für jede Suchlösung. Die erste Herausforderung besteht darin, die Daten so in das Suchsystem zu transferieren, dass diese später vernünftig durchsuchbar sind. Durchsuchbar meint in diesem Fall zum einen, dass die Inhalte vom Anwender gefunden werden können, aber auch dass die Inhalte zu einem erwarteten Zeitpunkt verfügbar sind.   Es gibt verschiedene Methoden, Inhalte aus Datenquellen wie Webseiten, CMS, DMS, File-System, SharePoint, Jive, Wikis, Datenbanken usw. in ein Suchsystem zu übernehmen.   Bei der Suche im Internet steht man häufig „nur“ vor der Aufgabe, ein System (CMS) durchsuchbar zu machen, und die darin gespeicherten Informationen sind in der Regel auch für alle Benutzer sichtbar.   Für eine Extranetsuche muss üblicherweise ebenfalls nur ein System berücksichtigt werden (CMS), aber hier gibt es bereits Benutzergruppen, die nur spezifische Inhalte sehen und natürlich auch nur diese durchsuchen dürfen.   Für die Suche im Intranet liegt meist die geballte Ladung an Varianten vor. Dort sind verschiedene Systeme, verschiedenste Dateiformate, granulare Berechtigungskonzepte und vieles mehr an der Tagesordnung.   CRAWLER   Die schnellste Art, aus Inhalten einen Suchindex zu erstellen, ist die Verwendung eines Crawlers. Dieser ist dem ersten Anschein nach prädestiniert für die Indizierung von Webseiten. Der Crawler ist auch die einzige Lösung, wenn man keinen Einfluss auf das zu durchsuchende System hat (z.B. die Webseite eines Mitbewerbers).   Allerdings ist ein Crawler die denkbar schlechteste Lösung, um Daten in einem Suchindex aktuell zu halten.   Der Crawler muss eine Webseite immer komplett durchforsten, um neue und geänderte Inhalte zu entdecken. Ein Crawler kann aufgrund der Beschaffenheit des Internets gelöschte Inhalte nicht direkt erkennen. Beim Prüfen einer bereits gecrawlten Seite erhält er vom jeweiligen Webserver nur die Meldung „Seite nicht gefunden“ zurück. Nun kann es aber sein, dass die Seite gar nicht gelöscht wurde, sondern aus technischen Gründen lediglich temporär nicht erreichbar ist. Daher muss ein Crawler eine nicht erreichbare (und eventuell gelöschte) Seite immer mehrmals prüfen, bevor diese auch als gelöscht erkannt und endgültig aus dem Suchergebnis entfernt wird.   Eine Suche für eine Webseite im Internet oder ein einfaches Extranet kann aber durchaus mit Hilfe eines Crawlers befüllt werden.   KONNEKTOREN   Konnektoren sind die erste Wahl, wenn Inhalte aus Quellsystemen extrahiert und zu einem Index aufgebaut werden sollen. Ein Konnektor läuft typischerweise innerhalb des Quellsystems und zapft dessen Workflows an.   Konnektoren kennen „ihr“ System, für das sie entwickelt wurden, und können daher die jeweiligen Features der Quellsysteme entsprechend einsetzen. Ein guter Konnektor erkennt Änderungen an seinem Quellsystem (sofern das Quellsystem dies zulässt) und kann direkt auf diese Änderung reagieren. Dies bedeutet, dass die Information über eine Neuanlage, eine Änderung oder eine Löschung direkt an das Suchsystem weitergeleitet werden kann. Somit ist der Suchindex beim Einsatz von Konnektoren im Gegensatz zum Crawling stets aktuell. SICHERHEIT Jeder Benutzer sollte im optimalen Fall nur die Dokumente in einem Suchergebnis sehen, die er auch sehen darf. Im einfachsten Fall dürfen alle Benutzer alle Dokumente sehen, suchen und finden.   Für einen Internetauftritt stellt sich die Frage nach der Sicherheit in der Regel nicht, da die Benutzer meist anonym sind und alle Inhalte auf der Webseite lesen dürfen.   Bei einem Extranet gibt es mindestens die Unterscheidung in öffentliche und nicht öffentliche Inhalte, es kann dort aber auch persönliche Inhalte geben.   Im Intranet sind Benutzerberechtigungen typischerweise am häufigsten und in den unterschiedlichsten Ausprägungen verbreitet.   Die Herausforderung für ein Suchsystem besteht darin, einem Benutzer genau die Dokumente anzuzeigen, die er auch sehen darf. Und das nicht nur bei der Verarbeitung der Suchanfrage, sondern auch bei der Ausführung von Funktionalitäten wie Autocomplete oder der Anwendung von Suchfiltern (Facetten)!   Early Binding   Um bei optimaler Performance die bestmögliche Treffermenge zu berechnen, ist ein so genanntes „Early Binding“ unerlässlich.   „Early Binding“ bedeutet, dass bereits bei der Indexierung die Berechtigungen der Dokumente mit im Suchsystem abgespeichert werden. Die Berechtigungen können dann zur Suchzeit sofort mit ausgewertet werden und garantieren somit eine sichere und schnelle Suche.   Late Binding   Unter „Late Binding“ versteht man das Prüfen und den Abgleich jedes Eintrags in der Suchergebnisliste mit dem Quellsystem.   Dabei wird versucht, anhand der Berechtigungsinformationen des aktuell suchenden Benutzers zu jedem Eintrag in der (technischen) Ergebnisliste im Hintergrund das entsprechende Dokument zu öffnen. Liefert das Quellsystem hierfür eine Fehlermeldung zurück (d.h. es ist keine Berechtigung vorhanden), wird das entsprechende Dokument in der Ergebnisliste des Benutzers nicht angezeigt.   Dieses Vorgehen ist zum einen langsam, zum anderen setzt es die Quellsysteme unter Last, da diese bei jeder Suchanfrage des Anwenders – auch bei bereits ausgeführten – immer die Inhaltsprüfungen beantworten müssen.   In der Praxis bedeutet dies: Im Optimalfall hat ein Benutzer das Recht, alle Inhalte zu sehen. Folglich wird bei einer Ergebnisliste von zehn Dokumenten genau zehnmal bei den Quellsystemen nachgefragt, ob ausreichende Berechtigungen vorliegen, und dem Benutzer werden diese Ergebnisse präsentiert. Hat ein Benutzer aber nur Rechte auf 10% der Inhalte, dann müssen schon 100 Anfragen an die Quellsysteme gestellt werden, um eine gültige Ergebnisliste für den Benutzer zu erstellen. BENUTZERGRUPPEN Die Benutzergruppen hängen natürlich sehr eng mit dem Thema Security zusammen. Bestimmte Benutzer oder Gruppen sollen nur Zugriff auf Inhalte erhalten, für die sie auch berechtigt sind.   Für ein einfaches Internetportal (z.B. eine kostenfreie News-Seite) gibt es nur eine Benutzergruppe: Alle. Jeder Benutzer darf alle Inhalte auf dem Portal sehen und somit auch alles suchen und finden. Eine Unterscheidung findet nicht statt (zumindest nicht über eine User-Authentifizierung).   Im Extranet sieht dies schon anders aus. Dort kann der Zugang zu Inhalten über die Zugehörigkeit zu Benutzergruppen (z.B. in einem Partnerportal: Hat der Benutzer den Status „Partner“ ja/nein?) oder über die Rechte der einzelnen Benutzer reglementiert sein. Hier kann es dann also durchaus der Fall sein, dass nicht jeder Benutzer alle Inhalte sehen darf. Allerdings gibt es in den meisten Fällen nur ein Authentifizierungssystem, das beachtet werden muss. Daher ist eine Implementierung einmalig zu machen, um eine rechteabhängige Suche und Ergebnisanzeige zu gewährleisten.   Im Intranet sind die Hürden schon wesentlich höher. Hier gibt es häufig mehrere Authentifizierungssysteme, oft auch durch Unternehmen selbst „gestrickt“. Dies stellt für eine optimal eingestellte Suche mit Berücksichtigung aller Quellsysteme und deren Authentifizierungselemente eine große Herausforderung dar. Ein entsprechendes Mapping auf die einzelnen Benutzer und Gruppen muss erstellt und bei der Indexierung und Suche berücksichtigt werden. In der Regel gilt: Je größer das Unternehmen, desto größer auch die Anzahl an Benutzern, Gruppen und Quellsystemen und damit auch der Koordinationsaufwand für eine rechtegeprüfte Suche. FAZIT Alles in allem werden die Anforderungen an ein Suchsystem und die angegliederten Systeme (Crawler, Konnektoren, Security-Mapper usw.) vom Internet über das Extranet bis zum Intranet immer komplexer.   Für jedes Szenario gibt es diverse Stellschrauben und Komponenten, die zu beachten und für die Qualität der Suche ausschlaggebend sind. Eine „click-and-run“-Installation ist ein passabler Anfang, aber die Tücken stecken für eine gut funktionierende Suche im Detail.   Der Autor Mit 30 Jahren IT- und Programmiererfahrung beschäftigt sich Jörg Issel, Principal Solution Manager der IntraFind Software AG, seit 1999 intensiv mit dem Thema Suche, insbesondere im heterogenen Unternehmensumfeld. &quot;Ich versuche immer, die für das Kundenproblem beste Lösung zu skizzieren und diese dann im Dialog mit den Kunden auch effizient umzusetzen. Die beste Software ist nutzlos, wenn sie die Probleme des Anwenders nicht löst.&quot;</body></Document><Document><url>https://www.intrafind.de/blog/approximative-data-structures-for-natural-language-processing-en</url><id>9E800D3572CB03DA3F2065692EC7C2E4</id><title>Approximative data structures for natural language processing</title><language>de</language><body>Approximative data structures for natural language processing   Some say software developers draw their motivation from minimizing or maximizing numbers in any given problem. That&apos;s a smug innuendo. From my experience, developers are always on the lookout for beautiful solutions, of which numbers are but a symptom. The usage of approximative data structures for language processing is one such example of a beautiful idea with nice numbers.   When dealing with natural language we&apos;re often confronted with the need of storing and retrieving language statistics, as e.g. n-Gram distributions, word counts, document frequency counts, etc. Inherent to the discrete nature of language, most of these statistics fall into a key-value schema, i.e. for a particular key (say, a word) we want to know a number. Let&apos;s focus on one particularly well known statistics, the document frequency (DF). The DF of a word is the number of documents in a corpus in which this word appears. The DF is used in the computation of the TFIDF, a simple yet proven heuristic for determining relevance.   To put things into perspective, the English Wikipedia has around 7Mio articles and the same order of magnitude of distinct words. If we were to use a hash map for storing the DFs, we would need approximately 100MB of memory just to store the words and counters (add at least another 100MB for data structure overhead). Thats for one language and one kind of statistics. One way to achieve lower memory requirements is to abdicate precision. This is a small price to pay, since we&apos;re dealing with corpus statistics, an intrinsically noisy affair.   Count-Min Sketch [C&amp;M 2005] is a sub-linear space data structure for computing approximate frequencies. It allows us to trade-off exactitude for memory. By tuning its two only parameters ( ϵ and δ ) we can answer the question of how many times we have seen a word within ϵ ⋅ | w | (where | w | is the number of distinct words in a corpus) of the actual value with probability δ . The memory needed for this will be proportional to 1 ϵ ⋅ - log 2 ( 1 - δ ) . Count-Min Sketches (CMS) share some similarities with bloom filters by using pairwise independent hash functions and sharing memory among keys. Figure 1 A CMS holds a two dimensional array of counters. The depth of the array determines the confidence probability ( δ ) and also the amount of hash functions we will use. The width determines the mean error we accept for count estimates. The two operations defined on a CMS are: add(key, value) and estimate(key). On add(key, value) we add value to the counters in the buckets corresponding to the computed hash values for each row using the independent hash functions (fig. 1). On estimate(key) we return the minimum value out of all corresponding buckets (fig. 2). As a side note: an easy improvement on the error expectation in detriment of a little runtime overhead is to change add(key, value) to only update the minimal buckets. Figure 2 One easy observation is that a CMS suffers under saturation, meaning that the precision guarantees depend on the number of distinct entries (words in our case); i.e. the more distinct words are added, the larger the chance of hash collisions distorting the counts. In a CMS errors are always positive, since there is no operation decreasing counter values. Yet another observation is that the errors have bigger impact on rare entries. Following the latter observation, CMS errors have a smoothing effect on word counts, which in NLP is something often required. As a cherry on the cake: CMSs have associative properties and can be elegantly distributed.   At IntraFind we have been using CMSs and other approximation techniques (e.g. sampling) in a few nlp applications with great success. We see approximative data structures as a vital technology block in a big data realm. At the present moment we are working on a novel approximative data structure, which we call a converging map, to store mappings from normal forms of terms or term compounds to their most frequent surface form appearing in a corpus, without the need of storing neither the normal forms nor the less frequent surface forms.     Bibliography: [C&amp;M 2005]: Cormode, G., &amp; Muthukrishnan, S. (2005). An improved data stream summary: the count-min sketch and its applications. Journal of Algorithms, 55(1), 58-75. Der Autor Breno Faria, Head of Development, ist seit 2012 für die IntraFind Software AG tätig. Seit den späten 2000er Jahren beschäftigt er sich intensiv mit den Themen Content Analytics und Information Retrieval. 2015 übernahm er die Rolle des Entwicklungsleiters bei IntraFind.   Im Rahmen von Veranstaltungen, z.B. Berlin Buzzwords 2014 oder &quot;IntraFind Enterprise Search Day 2015&quot;, referiert er regelmäßig über neue Technologien oder präsentiert innovative Lösungen aus IntraFind Kundenprojekten.</body></Document><Document><url>https://www.intrafind.de/blog/revolutioniert-die-semantische-suche-das-netz-trends-und-herausforderungen-in-der-forschung</url><id>BA6B654B9347A6B415F7D5DF4C28E16D</id><title>Revolutioniert die “Semantische Suche” das Netz? Trends und Herausforderungen in der Forschung.</title><language>de</language><body>Revolutioniert die “Semantische Suche” das Netz? Trends und Herausforderungen in der Forschung.   Der Begriff „semantische Suche“ wird sehr unterschiedlich und teilweise inflationär genutzt. Eigentlich fällt schon jedes Verfahren darunter, das die Benutzeranfrage (Query) in irgendeiner Weise interpretiert und versucht, eine optimale Antwort (keine reine Volltextsuche) darauf zu geben. Verfahren wie die „semantisch-assoziative Suche“ der IntraFind Software AG liefern verwandte Begriffe auf Basis der indexierten Dokumente, die zur Verfeinerung oder Erweiterung einer Suche dienen können.   Die Suche nach dem Begriff „Clinton“ liefert z.B. die möglichen Spezialisierungen „Hillary Clinton“, „Bill Clinton“ oder „Chelsea Clinton“, ohne dass diese Personen in einer Ontologie hinterlegt wären, rein auf Basis der vorhandenen Textdokumente. Jedoch wird auch der Begriff „Weißes Haus“ als mögliche Erweiterung der Suche geliefert. Das Verfahren ist vergleichbar zu Clustering-Techniken. Eigentlich wird eine Tag Cloud zur aktuellen Suche geliefert.   Vorteil: Eine manuelle Pflege ontologischer Ressourcen ist nicht notwendig, da das Verfahren rein auf statistischer Basis, jedoch unter Berücksichtigung linguistischen Wissens (Wortkategorien, Noun Phrase-Erkennung) arbeitet. Einen ähnlichen Effekt kann man durch Einbeziehung eines manuellen Thesaurus erzielen. Auf dieser Basis kann sogar sprachübergreifend (crosslingual) gesucht werden.   In letzter Zeit prägen Google und Siri den Begriff “semantische Suche”.   Benutzeranfragen werden interpretiert und anstelle einer Trefferliste werden wirkliche Antworten generiert, zumindest bei Faktenfragen wie der Frage nach einem chinesischen Restaurant in der Nähe oder nach dem Geburtsdatum der Bundeskanzlerin. Bei nicht eindeutig interpretierbaren Faktenfragen wird ein Artikel der Wikipedia zum Hauptsuchbegriff zurückgeliefert. Faktenfragen lassen sich schon mit relativ einfachen Verfahren (Wer, Wo, Wann, Wie groß, Wie viel, …) erkennen und interpretieren. Sie werden in den meisten Fällen durch Einträge aus Datenbanken (oder Triple Stores wie der dbpedia) beantwortet. Ohne diese strukturierte Information aus Datenbanken würde die „semantische Suche“ á la Google und Siri nicht funktionieren.   Besonders beeindruckend sind natürlich solche Ergebnisse, wenn gleichzeitig der aktuelle Ort des Fragestellers mit einbezogen wird oder die „semantische Suche“ mit einer Spracherkennung verbunden ist. D.h. die hinter der semantischen Suche von Google und Siri steckende Technologie ist kein Hexenwerk. Echtes Textverständnis auf Basis von intelligenten Verfahren kommt nicht zum Einsatz.   Da jedoch in der Praxis oft die strukturierten Daten fehlen, versuchen wir bei IntraFind bzgl. „semantischer Suche“ etwas weiterzugehen. Wir erkennen Entitäten wir Personen, Organisationen und Orte in Texten. Schon vor 4 Jahren haben wir eine semantische Suchmaschine gebaut, die Faktenfragen rein auf der Basis von Text beantworten kann. Wie oben geschildert, werden Faktenfragen auf Basis einfacher Muster erkannt.   So wird die Frage nach den Gründern von Microsoft („Wer hat Microsoft gegründet?“) übersetzt in eine Query nach Personen in der Nähe des Begriffs “Microsoft” und Synonymen des Wortes „gründen“. Damit lässt sich die Frage auch ohne eine Datenbank mit Faktenwissen und ohne aufwändig manuell gepflegte Ontologien beantworten.   Aus meiner Sicht sind derartige Ansätze notwendig, um beim Thema „semantische Suche“ weiterzukommen, denn für die meisten interessanten Fragen gibt es keine manuell gepflegte Datenbanken, sondern nur textuelle Information. Für einen wirklichen Erfolg der semantischen Suche brauchen wir Verfahren, die Faktenwissen aus Texten extrahieren. Dies ist die Herausforderung für die nächsten Jahre.   Der Autor Dr. Christoph Goller verfügt über 15 Jahre Erfahrung im Enterprise Search Markt. Er promovierte in Computerwissenschaften an der Technischen Universität in München und arbeitete im Anschluss in einigen wissenschaftlichen Projekten zu Themenbereich Künstliche Intelligenz, Machine Learning sowie Neuronale Netze.   Seit 2002 leitet er den Bereich Forschung bei IntraFind und verantwortet IntraFind&apos;s Kern-Suche- und Content Analyse Technologien. Zwischen 2003 und 2007 arbeitete Christoph aktiv als Committer in Apache&apos;s Open Source Projekt Lucene.    </body></Document><Document><url>https://www.intrafind.de/blog/contract-analyzer-vertraege-analysieren-mit-due-diligence-software</url><id>4E0E831876D63BE6F69CA663968FE405</id><title>Contract Analyzer: Verträge analysieren mit Due Diligence Software</title><language>de</language><body>Contract Analyzer: Verträge analysieren mit Due Diligence Software Contract Analyzer: Verträge analysieren mit Due Diligence Software Es sollte eine der größten Übernahmen in der Firmengeschichte werden: Als Rolf M. den Vertrag unterzeichnete, schien das Geschäft perfekt. Mit dem Zukauf der anderen Firma wollte Rolf M. mit seinem Unternehmen in neue Marktfelder vordringen und das Produktportfolio erweitern. Doch eines Morgens kam mit einem einfachen Brief das böse Erwachen: Wie sich herausstellte, gehörten die Markenrechte des relevanten Produkts von der neu übernommenen Firma längst einem anderen Unternehmen. Das Unternehmen von Rolf M. konnte nicht mehr davon profitieren. Der Schaden ging in die Millionenhöhe. Mit Due Diligence Software Risiken erkennen Steht eine Firma oder Organisation zum Verkauf oder spielt ein Unternehmen mit dem Gedanken, ein anderes Unternehmen zu übernehmen, fallen nicht nur betriebswirtschaftliche Entscheidungen an. In sogenannten Due Diligence Verfahren wird geprüft, welchen Wert das Unternehmen hat und welche laufenden Verträge es gibt. Dabei kommen Fachjuristen oder Anwälte ins Spiel, die oftmals für spezielle Firmen für Merger &amp; Acquisitions (M &amp; A) tätig sind. Sie überprüfen, ob es zum Beispiel noch laufende Gerichtsverfahren gibt oder welche bestehenden Kauf- und Mitarbeiterverträge existieren. Bei solchen Due Diligence Verfahren geht es darum, Risiken aufzuspüren, die sich durch die Übernahmen ergeben könnten. Im Falle von Rolf M. waren dies Patent- und Markenrechtsvereinbarungen, die einige Jahre zuvor geschlossen wurden und von den Anwälten wohl übersehen wurden. Due Diligence- und M &amp; A-Verfahren finden oftmals unter sehr hohem Zeitdruck und begrenztem Budget statt. Fachjuristen müssen dabei seitenlange Verträge durcharbeiten. Bei einer 60-Millionen-Transaktion müssen Anwälte sich schätzungsweise durch etwa 40.000 Dokumente durcharbeiten. Dabei fallen enorme Kosten an: Die Gesamtkosten, die weltweit durch Due Diligence verursacht werden, werden auf 90 Milliarden Dollar geschätzt. Durchschnittliche Stundensätze für Fachjuristen von 350 Euro sind dabei keine Seltenheit. Mit Artificial Intelligence Risiken und Kosten minimieren Die Software Contract Analyzer soll Anwälte künftig bei ihrer Arbeit unterstützen und Menschen wie Rolf M. rechtzeitig davor warnen, wenn sie unnötige Risiken eingehen. Die Anwendung basiert auf Artificial Intelligence und unterstützt Machine Learning Verfahren. Dadurch ist sie in der Lage, bestimmte Klauseln, die kritisch sind, in den Verträgen zu erkennen und farbig zu markieren. Der Fachjurist spart sich Zeit, weil ihm die Software das seitenlange Lesen abnimmt. Er kann sich hingegen auf den wesentlichen Teil seiner Arbeit konzentrieren, nämlich das Prüfen und Bearbeiten der einzelnen Klauseln. Für das Unternehmen minimieren sich wiederum die potenziellen Risiken und auch die Kosten, da das Prüfen der Verträge mit weniger Aufwand verrichtet werden kann. Über das zentrale Dashboard des Contract Analyzer sieht der Fachjurist auf einem Blick, welche Dokumente ihm zugeordnet sind. Die kritischen Stellen des Vertrags erkennt er anhand eines Red Flag Reports. Das Management sieht ebenfalls über das Dashboard, bei welchen Verträgen es Punkte gibt, die gegebenenfalls nachverhandelt werden müssen. Der Contract Analyzer ist komplett browsergesteuert. Egal ob Netzlaufwerke, Dokumentenmanagementsysteme oder Wikis – der Contract Analyzer verfügt über zahlreiche Konnektoren und kann an verschiedene Datenquellen angebunden werden. So kann er alle wesentlichen Verträge analysieren, egal an welchem Ort sie vorher gespeichert worden sind. Es ist außerdem möglich, gescannte Dokumente über eine integrierte OCR-Schnittstelle einzubinden und zu analysieren. Zusätzlich unterstützt der Contract Analyzer über 600 Dateiformate. Dem Nutzer wird dadurch eine Vorschauansicht auf das wesentliche Dokument ermöglicht, ohne dass die entsprechende Software, wie beispielsweise Microsoft Visio, auf seinem Gerät installiert sein muss. Über die Benutzeroberfläche kann der Fachjurist auch nach bestimmten Dokumenten suchen. Die Ergebnisse in der Trefferliste sind dabei rechtegeprüft. Das bedeutet, dass er nur die Dateien sieht, die er auch berechtigt ist zu sehen. Hat der Fachjurist die jeweiligen Dokumente geprüft, kann er sie freigeben und zu einem Report hinzufügen. Andere Projektverantwortliche können diesen Status wiederum über ihr eigenes Dashboard mitverfolgen. Mit dem Contract Analyzer kann eine Firma somit nicht nur erfolgreich Risiken minimieren, sondern auch effizienter arbeiten und Geld sparen. Haben wir Ihr Interesse geweckt? Dann nehmen Sie mit uns Kontakt auf und fragen uns nach dem Contract Analyzer. Mehr Informationen finden Sie auch unter AnalyzeLaw.com Über die Lösungen zu Artificial Intelligence von IntraFind finden Sie hier weitere Details.   Der Autor Robert Eberhard ist Diplom-Informatiker und kann auf über 20 Jahre Berufserfahrung zurückblicken. Seine Schwerpunkte sind Suchmaschinenoptimierung und IT-Projektmanagement.</body></Document><Document><url>https://www.intrafind.de/blog/einfuehrung-einer-unternehmensweiten-suche-aller-anfang-ist-nicht-schwer</url><id>99A05BD83E3DC4D90412445AC1AF1326</id><title>Einführung einer unternehmensweiten Suche: Aller Anfang ist (nicht) schwer</title><language>de</language><body>Einführung einer unternehmensweiten Suche: Aller Anfang ist (nicht) schwer   Die Einführung einer unternehmensweiten Suche ist und bleibt ein IT-Projekt, birgt somit eine gewisse Komplexität und sollte daher gut geplant sein.   Enterprise Search hat viele Facetten: Es soll die Suche und Darstellung von Zusammenhängen - z.B. im Bereich Produktdatenmanagement - ebenso unterstützen wie die Klärung von Fragestellungen im Vertragsmanagement, im Marketing, im Vertrieb und allen anderen Abteilungen. Der Nutzen einer Search-Lösung liegt schließlich darin, den Informationsschatz eines Unternehmens allumfassend in den Griff zu bekommen und – sofern freigegeben- möglichst vielen Mitarbeitern zur Verfügung zu stellen.   Aus jahrelanger Projekterfahrung heraus empfehle ich Ihnen jedoch, sich dabei nicht zu übernehmen („think big“), sondern mit einem iterativen Vorgehen zunächst Erfahrungen in einem kleineren Projekt zu sammeln („start small“). Denn schon dort werden Sie erste Hürden nehmen müssen, mit denen Sie auch später im größeren Rahmen konfrontiert sein werden.   Sind diese ersten Stolpersteine jedoch aus dem Weg geräumt, ist die Suche in Teilbereichen erst einmal eingeführt und der erste Druck aus den Fachabteilungen verschwunden, erhalten Sie genau das Schulterklopfen, aus dem die Motivation hervorgeht, auch komplexe Dinge anzugehen.   Also klein anfangen und dann wachsen! Starten Sie beispielsweise mit der Anbindung einer Datenquelle an die Suche und erweitern Sie sukzessive um weitere Datenquellen.   Besonders sinnvoll als erster Angriffspunkt: die File-Server. Hier schlummert in der Regel ein großer Teil des Wissens eines Unternehmens. Hier liegt der richtige strategische Ansatz, hier bietet sich viel Raum für Experimente und erste Erfahrungen. Der Aufwand ist überschaubar, das Effizienzsteigerungspotential enorm und Sie erhalten einen guten Eindruck über die Qualität der Suchmaschine und wie solch ein System von den Mitarbeitern angenommen und in den täglichen Arbeitsprozess integriert wird.   Tragen Sie sich mit dem Gedanken, eine übergreifende Suche in Ihrem Unternehmen einzuführen? Es werden unzählige Fragen auftauchen: Wie gehen wir vor? Wie viel Budget brauchen wir? Wie viele Projektmitglieder? Für welches Produkt entscheiden wir uns? Welche Datenquellen sollen an die Suche angebunden werden? Wie und wo verankern wir sie? Wie sehen Oberfläche und Trefferliste aus? Und, und, und.   Vergleichbar mit dem Bau eines Hauses, das man auch nicht ohne Architekt baut, fällt es mit ein bisschen Hilfe von extern meist leichter, sich in diesem Neuland zu bewegen.   Die wichtigste Frage lautet doch:   Was muss ich berücksichtigen, damit das Projekt „Unternehmensweite Suche“ ein Erfolg wird?   Für das Internet lässt sich das leicht beantworten. Die unterschiedlichsten kommerziellen Suchen haben alle eins gemeinsam: sie liefern als Ergebnis das, was der Benutzer im Rahmen seiner Aufgabenstellung sucht. Die Erwartungen der Nutzer werden somit erfüllt.   Übertragen auf Ihr Unternehmens-oder Behördenumfeld muss die Suchmaschine also sowohl bei generellen Suchanfragen (denken Sie an den Speiseplan der Kantine oder an Vorlagen für Urlaubsanträge etc.) als auch bei hochspezialisierten und sehr unterschiedlichen Recherchen („Welche Kundenbeschwerden gab es im letzten Quartal zu Produkt X und wurden deshalb Änderungen am Produkt vorgenommen?“) unterstützen und zufriedenstellende Ergebnisse liefern. Entwicklung, Produktion, Personal, Finanzen, die Art der Anfragen kann dabei unterschiedlicher nicht sein.   Aus der Sicht einer Privatperson betrachtet, sucht man kaum eine Wohnung, ein Auto oder einen konkreten Musiktitel über eine der großen Internetsuchmaschinen. Ich begebe mich stattdessen auf ein spezielles Immobilienportal, eine Autobörse oder auf das Portal eines Musikanbieters. Andererseits kann ich jedoch kaum erhoffen, von einem Immobilienportal einen Überblick über die besten, derzeit auf dem Markt befindlichen Fernsehgeräte und deren innovative Funktionen zu erhalten.   Doch genau diese beiden Typen von Anforderungen gilt es mit einer Suchmaschine für ein Unternehmen zu erfüllen. Die Alternative, eine Fülle an Suchtechnologien je Applikation, Abteilung oder Art der Frage zu beschaffen, ist keine Option, zumal hier für Betrieb und Wartung hohe Kosten anfallen würden, die solche Projekte schnell unwirtschaftlich machen.   Ein erfolgreich eigeführtes Suchsystem trifft die unterschiedlichen Bedürfnisse der Nutzer und liefert dabei vollständige und qualitativ hochwertige Suchergebnisse - gleichgültig, ob eine zentrale, allgemeine Suche (Speiseplan) oder eine spezialisierte Recherche ausgeführt wird. Die Art und Weise, wie die User an das System herangehen, ist dabei für alle Arten der Suche gleich.   Der Grundstock hierfür ist eine Lösung, in der alle relevanten Inhalte für alle Benutzer mit der Suchmaschine indiziert und in der Suche bereitgestellt werden (können); wie auch immer die Bereitstellung später aussieht. Dies bedeutet, dass erst einmal alle notwendigen Datenquellen der Organisation in der Suche erfasst und zur Verfügung gestellt werden müssen.   Denn nur dann bin ich der Lage, auf die verschiedenen Bedürfnisse der Benutzer zu reagieren. Ich kann beispielsweise verschiedene Datentöpfe miteinander kombinieren, ohne Qualitätseinbußen in der Suche hinnehmen zu müssen und kann sicherstellen, dass Benutzer nur eine Suchsyntax lernen müssen. Dank einer einheitlichen intuitiv bedienbaren Benutzeroberfläche besteht hier jedoch kein großer Schulungsbedarf.   Grundlegende Funktionen und Schnittstellen   Welche grundlegenden Funktionen muss eine Suche dafür haben?   Dazu gehören:   Fertige Such- und Ergebnisseiten, die anpassbar sind, um sie im Intranet oder Internet in einem Portal anzubieten; dies ermöglicht die schnelle Bereitstellung einer Suche für das Unternehmen, ohne sich gleich um eine komplexe Integration in bestehende Portale kümmern zu müssen. Die Möglichkeit, die Suche oder Komponenten davon in ein bestehendes Intranet oder Web einzubinden - der logische nächste Schritt, wenn bereits Portale im Unternehmen bestehen. Funktionen und Schnittstellen, um die Suche in bestehende Anwendungen zu integrieren. Hier wird es spannend, denn daran denkt man in der Regel erst später. Sogenannte „Konnektoren“, die es mir erlauben, die verschiedenen im Unternehmen vorhandenen Datenquellen in die Suche einzubinden. Die Möglichkeit, gegebenenfalls Suchanfragen an andere Suchmaschinen weiterzuleiten und deren Ergebnisse zu verarbeiten. Dies ist notwendig, wenn Informationen aus dem Internet einbezogen werden sollen Nicht zu vergessen, der entscheidende Erfolgsfaktor:   Die Fähigkeit, Zugriffsrechte bei der Indexierung, bei der Suche und auch in der Oberfläche zu berücksichtigen! Nicht jeder Benutzer darf und soll sämtliche, zu seiner Suchanfrage im Unternehmen vorhandenen und verfügbaren Informationen in einer Trefferliste sehen.   Oft vergessen: Navigationselemente und Steuerelemente einer guten Suchoberfläche werden häufig aus dem durchsuchten Inhalt gebildet. Über eine rechtegeprüfte Autovervollständigung ist hier zu steuern, welche Inhalte der Benutzer sehen darf und welche nicht.   Erfüllt die ausgewählte Suchmaschine all diese Kriterien, beginnt das nächste große Kapitel des Abenteuers Suchmaschineneinführung: wie stelle ich meinen Benutzern die Informationen zur Verfügung?   Darüber dann mehr im nächsten Blog-Beitrag.   Der Autor Nach dem BWL Studium war Rutger Lörch in verschiedenen Positionen bei namhaften Hardware- und Softwareanbietern wie Nixdorf, Digital und Oracle tätig. Seit 2008 unterstützt er beim Suchspezialisten IntraFind den Vertrieb. „Das Thema Enterprise Search ist deshalb so faszinierend, weil die Anforderungen der jeweiligen Interessenten sehr individuell sind. Ein sehr abwechslungsreiches Betätigungsfeld voller spannender Herausforderungen.“</body></Document><Document><url>https://www.intrafind.de/blog/was-2017-wichtig-wird</url><id>EBE5FBD5306083BC186140E9B2C9FC2A</id><title>Was 2017 wichtig wird</title><language>de</language><body>Was 2017 wichtig wird Die wichtigsten IT-Trends für Enterprise Search Lösungen Neues Jahr, neue Trends: Kaum waren die letzten Silvesterböller verschossen, trafen sich die ersten IT-Experten Anfang 2017 schon wieder zu Konferenzen und Messen wie beispielsweise zur CES in Las Vegas. Artificial Intelligence war dort eines der zentralen Themen. Außerdem wurden zahlreiche neue Gadgets vorgestellt, die im Laufe des Jahres wohl auch in den deutschen Märkten zu finden sein werden. An Prognosen über neue Produkte und Techniktrends mangelt es nicht. Laut dem Analystenhaus Gartner werden Themen wie Internet of Things, Machine Learning und Big Data auch 2017 die wichtigste Rolle in der IT-Branche spielen. Gartner sieht dabei die wesentlichsten Entwicklungsschübe bei den intelligenten Apps, Smart Cars und Chatbots. Machine Learning wird insbesondere den Betrieb der Rechenzentren vorantreiben, die Automatisierung der Systeme wird noch mehr zunehmen, so dass sie in einem Störungsfall proaktiv reagieren können. Die Rechenleistung verbessert sich immer weiter, das Datenaufkommen steigt und Algorithmen werden immer ausgereifter. Die Weiterentwicklung von Technologien wird Machine Learning und Deep Learning weiteren Auftrieb geben. So sollen digitale Assistenten wie Cortana und Siri künftig Verhaltensmuster analysieren, verstehen und Entscheidungen daraus ableiten können.      Das hat auch Auswirkungen auf Enterprise Search. Professionelle Enterprise Search Software ist nicht mehr nur auf das reine Suchen und Finden von Daten beschränkt. Durch zahlreiche semantische Funktionen, linguistische Features und Verfahren wie Natural Language Processing (NLP), das Sprache verarbeitet und daraus Erkenntnisse ableitet, ist die Software in der Lage, den Nutzer zu „verstehen“ und aus unstrukturierten UND strukturierten Daten Informationen zu gewinnen und in Kontext zu setzen. IoT, Big Data oder Deep Learning werden die Funktionalitäten von Enterprise Search Software maßgeblich beeinflussen. Was wird 2017 aus unserer Sicht besonders wichtig? Wir haben bei den Kollegen aus der Vorstandsetage, dem Marketing und der Entwicklung nachgefragt: Franz Kögl, Vorstand IntraFind Software AG: „Tiefes Textverständnis ist für mich das entscheidende Thema für 2017. Anwender wollen die Daten nicht einfach nur präsentiert bekommen, sondern wollen deutlich mehr aus den bestehenden Informationen herausholen. Auch wird sich die Art und Weise, wie gesucht wird, mit natürlichsprachlicher Suche deutlich verändern. Enterprise Search Engines müssen sich zu Insight Engines entwickeln, die dem Anwender wichtige Erkenntnisse liefert. Eine professionelle Enterprise Search Software wird ohne Verfahren wie Natural Language Processing und Deep Learning nicht mehr auskommen.“ Dr. Christoph Goller, Head of Research IntraFind Software AG: „Machine Learning Verfahren wie Neuronale Netze und auch Deep Learning werden 2017 weiterhin eine große Rolle bei uns spielen. Der jetzige Hype um diese Themen amüsiert mich wirklich ... wir machen das schon seit über 10 Jahren und nutzen die Verfahren in unseren Produkten und Lösungen Schön zu sehen, dass unser &quot;Tägliches Brot&quot; im Mainstream angekommen ist.“ Sonja Bellaire, Marketing Managerin bei IntraFind Software AG: „IT betrifft uns alle. Software muss daher so einfach wie möglich zu bedienen und zu verstehen sein. Darstellungsmöglichkeiten wie der Knowledge Graph bieten hierfür eine prima Möglichkeit.“ Breno Faria, Head of Development, IntraFind Software AG: „Technologien rund um Roboter, die Arbeitsschritte übernehmen oder Chatbots, die automatisch antworten, werden sich 2017 erheblich weiterentwickeln. Das spielt auch der Enterprise Search Software in die Hände. Künftig weiß die Software anhand meines Kalenders, dass ich mich gerade auf dem Weg in ein Meeting befinde und liefert mir alle Dokumente, die ich dafür brauche.“             Der Autor Christiane Stagge studierte Geschichte und Politikwissenschaft und ist seit 2006 als IT-Redakteurin tätig. Seit 2016 unterstützt sie die IntraFind Software AG in der Unternehmenskommunikation.   </body></Document><Document><url>https://www.intrafind.de/blog/tipps-tricks-fuer-ihr-erfolgreiches-enterprise-search-projekt</url><id>D5F3DEB560A17366912679F83632179C</id><title>Tipps &amp; Tricks für Ihr erfolgreiches Enterprise Search-Projekt</title><language>de</language><body>Tipps &amp; Tricks für Ihr erfolgreiches Enterprise Search-Projekt   Aller Anfang muss nicht schwer sein - kann aber ...   Im Verlauf von 14 Jahren Praxiserfahrung - vom Erstgespräch bei einem Interessenten bis hin zum Ausbau einer Suchlösung im produktiven Betrieb - war ich mit einer Vielzahl unterschiedlicher Projekte konfrontiert: angefangen von rein IT-getriebenen Themen bis hin zu Projekten, die eine Fachabteilung ohne Einbindung der IT-Abteilung in Eigeninitiative umgesetzt hatte.   Das Buying Center eines Enterprise Search-Projektes setzt sich aber sinnvollerweise aus beiden &quot;Lagern&quot; zusammen: Die IT allein vernachlässigt meist die Wünsche und Bedürfnisse der Benutzer, die Fachabteilung bekommt oft hinterher Probleme, wenn die IT zu spät an Bord geholt und nicht von Anfang an eingebunden wurde. Auch haben wir es bei unseren Kunden mit einer großen Bandbreite an Expertise für Search und Content Analytics zu tun: Fachabteilungen, die wild mit Buzzwords wie „Semantische Suche“ oder „Semantische Netze“ um sich werfen und bei Nachfrage nicht sagen können, was damit im konkreten Fall gemeint ist, bis hin zu Experten, die mit mir auf Expertenniveau die Details der Funktionsweise unserer Linguistik diskutieren.   Use Cases   Das bringt uns zu einem ersten wichtigen Kriterium für ein erfolgreiches Suchprojekt: Gute Planung im Vorfeld. Den Aspekt, ein vernünftiges Projekt mit professionellem Projektmanagement aufzusetzen, sich um das Hardware-Sizing oder um Authentifizierungsfragen / Single Sign On-Themen für rechtebasierte Suche zu kümmern, lassen wir jetzt außen vor. Es geht mir hier um die oftmals sträflich vernachlässigten Use Cases bzw. eine präzise Anforderungsanalyse.   Ich höre oft Sätze wie „Wir brauchen eine interne Google-Suche&quot; - und das war´s dann mit den Anforderungen. Dabei wäre es so einfach, mit normalen Benutzern und &quot;Wissensarbeitern&quot; zu sprechen, welche Fragen sie in Ihrem täglichen Arbeitsumfeld zu beantworten haben, wie sie heute nach welchen Informationen suchen und wie genau dieser Prozess verbessert werden kann. Vielleicht stellen Sie im Gespräch mit Ihren SAP-Usern fest, dass es für Ihre Kollegen nicht sinnvoll ist, SAP-Inhalte aus dem Rechnungswesen in einer Enterprise Search auffindbar zu machen, weil die &quot;Heavy-SAP-User&quot; nie im Leben auf die Idee kämen, ihre Rechnungen nicht mit SAP-Bordmitteln zu suchen. Diese Experten kennen ihre Buchungskreise und die oftmals verschlungenen Pfade zu den gesuchten Dokumenten - und schon haben Sie Ihr Suchprojekt erheblich vereinfacht.   Benutzer frühzeitig zu involvieren und zu befragen, hat den Vorteil, später im produktiven Betrieb auch einen Vergleich anstellen zu können, was sich konkret verbessert hat. Die Investition in ein Enterprise Search-Projekt muss wie bei allen Projekten irgendwann gerechtfertigt werden. Suchprojekte haben - wie im vorherigen Blogbeitrag bereits dargestellt - oftmals das Problem eines schlecht messbaren ROIs. Anhand der konkreten Anforderungen der Benutzer kann dann aber später belastbar die qualitative Verbesserung dargestellt werden.   Think big - start small   Ein wichtiger Tipp: Unterschätzen Sie niemals die Komplexität eines Suchprojektes! Eine Lösung für die Suche im Dateisystem ist schnell installiert und konfiguriert. Je mehr Datenquellen aber in die unternehmensweite Suche mit eingebunden werden, desto komplexer wird die technische Seite des Projektes. Die verschiedenen Datensilos müssen angebunden werden - der indizierungs- und suchseitige Zugriff auf die Inhalte muss schnell, rechtegeprüft und updatefähig erfolgen und dementsprechend konzipiert werden. Je mehr Quellen, desto mehr Vorarbeit. Wir unterscheiden hier zwischen einfachen Quellen, die &quot;out-of-the-box&quot; angebunden werden, dann Repositories wie Lotus Notes oder Microsoft Exchange, bei denen die Konnektoren konfiguriert werden müssen, und Datentöpfen wie SAP NetWeaver-Portalen, bei denen oftmals die Konnektoren an die kundenspezifische Installation des Datentopfs angepasst werden müssen.   Haben Sie alle für die Recherche Ihrer Mitarbeiter relevanten Datenquellen auf dem Schirm? Vergessen Sie z.B. keine sinnvollen externen Quellen wie z.B. Fachinformationsanbieter oder Social Media-Inhalte - konzipieren Sie sich aber nicht &quot;zu Tode&quot;. Eine schrittweise Einführung - beginnend mit den einfach anzubindenden Datenquellen - versetzt Sie in nur kurzer Zeit in die Lage, eine Suchlösung produktiv zu nutzen. Eine sukzessive Erweiterung um weitere Datenquellen rundet dann die Lösung ab. &quot;Der Geschmack kommt beim Essen&quot; trifft hier voll und ganz zu. Um auch hier wieder ein Buzzword zu verwenden: Realisieren Sie diesen &quot;Quick Win&quot;, holen Sie Ihre User an Bord und bauen Sie dann die Lösung durch die Anbindung weiterer Datenquellen aus. Think big - but start small.   Nicht nur Quantität zählt   Doch nicht nur die Hinzunahme weiterer Datentöpfe, sondern auch das Ausrollen von z.B. &quot;Spezial-Suchapplikationen&quot; ist eine weitere sinnvolle Ausbaumöglichkeit: Die Enterprise Search-Lösung ist auch in der Lage, die vorhandene Suche in Applikationen wie dem Produktdatenmanagement-System qualitativ aufzuwerten oder zu ersetzen oder kann für Compliance-Zwecke eingesetzt werden. Gerade eben realisieren wir für die Ingenieurs- und Entwicklungsbereiche eines langjährigen Kunden eine für diese Usergruppe maßgeschneiderte Suchlösung - mit einer an die speziellen Rechercheszenarien dieser Benutzer angepassten Suchoberfläche. Das Indizierungsbackend ist dabei dasselbe, das auch die unternehmensweite Suche bedient. Dieses zentrales Indizierungs- und Such-Backend fungiert als zentrale Wissensdatenbank, mit der unterschiedliche Benutzergruppen im Unternehmen ganz einfach individuell optimierte Such-Sichten für ihre jeweilige tägliche Arbeit nutzen können.   Sehen Sie Suche mal aus diesem etwas abstrakteren Blickwinkel: Indizierungs- und Such-Backend sind leistungsfähige Infrastrukturkomponenten, die unterschiedlichste Aufgaben erledigen. Von der Such-/Trefferseite im Intranet über einen Point of Knowledge für Ihren Vertriebsmitarbeiter, wo er aus verschiedenen Systemen alle Informationen zu seinem Kunden in einem Dashboard präsentiert bekommt bis hin zu einer Lösung für die Analyse der Bewertungen Ihres Unternehmens, Ihres Managements oder Ihrer Produkte in sozialen Medien durch Sentiment-Analyse. Bei dem anfangs genannten Kundenbeispiel binden wir zu ausgewählten internen Inhalten auch externe Patent- und Literaturdatenbanken an. Der User sieht in der Suchmaske sogar, ob ein bestimmtes Fachbuch oder eine Studie bereits bestellt wurden und wer im Unternehmen das Medium gerade nutzt.   Diese vielfältigen Einsatzmöglichkeiten können nicht vorab im Rahmen einer Lasten-/Pflichtenheftdefinition umfänglich konzipiert und umgesetzt werden. Ein derartiges Unterfangen würde vermutlich scheitern. Die schrittweise Umsetzung ist hier der Königsweg. Was wichtig dabei ist: Die Iterationen nach einer Initial-Installation sind kleine, überschaubare Projekte.   Last, but not least   Erfolgreiche Suchprojekte zeichnet zudem aus, dass der Produktivstart eines neuen Suchdienstes intern vermarktet werden muss. Unsere Kunden sind hier sehr kreativ: Angefangen bei Anzeigen im Intranet, Beiträgen in internen Newslettern und Videos bis hin zu Preisausschreiben, bei denen die User bestimmte Suchfunktionalitäten ausprobieren müssen, bietet sich eine breite Palette an möglichen Maßnahmen.   Einige unserer Kunden haben auch pro Abteilung einen &quot;Peer-User&quot; benannt, der als direkter Ansprechpartner für anfängliche Fragen zur Verfügung steht. Benutzer sind oft überrascht, welche komplexen Suchanfragen doch ganz einfach von der Maschine &quot;beantwortet&quot; werden können. Ein direkter Ansprechpartner - zusätzlich zum generellen IT-Support - kann hier nützlich sein, damit die User die Möglichkeiten und Potentiale der neuen Lösung schnell &quot;hands-on&quot; kennenlernen. Auf der Tonspur kommt meist doch mehr rüber als im besten Erklärvideo.   Ein besonderes Anliegen ist mir noch der Hinweis auf die Notwendigkeit, den produktiven Betrieb nicht nur aus der Sicht eines IT-Administrators zu betrachten, sondern auch die Rolle eines zentralen &quot;Search Competence Centers&quot; zu definieren. Eine Person reicht hier schon aus: Jemand, bei dem alles zum Thema Suche und Content-Analyse zusammenläuft und der sich auch um die fachliche Administration kümmert. Der sich beispielsweise die Suchanfragen anschaut, die zu keinen Treffern führten und dann ggf. Synonyme einpflegt, damit die User bei der Suche nach dem &quot;Führerschein&quot; auch das erwartete Dokument finden, in dem leider nur der Begriff &quot;Fahrerlaubnis&quot; steht. Diese zentrale Stelle hilft, dass das &quot;Such-Rad&quot; nicht in jeder Abteilung neu erfunden werden muss, sondern dass es einen erfahrenen Ansprechpartner gibt, der die Themenhoheit innehat.   Was neben diesen grundsätzlichen Themen noch wichtig ist, um ein Suchprojekt erfolgreich zu machen, lesen Sie im nächsten Beitrag.   Der Autor Franz Kögl ist Mitgründer und -inhaber der Firma IntraFind Software AG und verfügt über mehr als 15 Jahre Erfahrung im Enterprise Search und Content Analytics Bereich. Mehr über Franz Kögl finden Sie in unserem Management Profil  </body></Document><Document><url>https://www.intrafind.de/blog/von-der-enterprise-search-zur-insight-engine</url><id>2C76225C22FA01F141DCE13A9EA29452</id><title>Von der Enterprise Search zur Insight Engine</title><language>de</language><body>Von der Enterprise Search zur Insight Engine Suchmaschinen als Treiber fürs Wissensmanagement Ob Kundenadressen, Sensordaten oder Produktfactsheets – auf den Unternehmensrechnern sammeln sich immer mehr Daten an. Die Digitalisierung macht es möglich: Durch Smartphones, verbesserte Webtechnologien oder Social Media kommunizieren Firmen heutzutage viel häufiger mit Kunden, externen Mitarbeitern oder Lieferanten,  als es noch vor einigen Jahren der Fall war. Die dabei gewonnen  Informationen sind eine wahre Goldgrube – und das gilt nicht nur für Kundendaten. Auch innerhalb eines Unternehmens sammelt sich Wissen an, das in Form von Dateien, Worddokumenten, PDF-Files oder Tabellen auf den einzelnen Rechnern gespeichert ist, ohne dass die Kollegen der anderen Abteilungen davon wissen und deren Potenzial wirklich ausschöpfen.     IoT als Treiber vom Datenwachstum Besonders das Internet der Dinge (IoT) wird das Datenwachstum vorantreiben. Wenn Sensoren miteinander kommunizieren und Daten miteinander austauschen, wird das die Produktion nachhaltig verändern.  Laut einer Studie von Gartner („1000 Data and Analytics Predictions Through 2020“) stammt in vier Jahren ein Viertel der Daten aus IoT-Anwendungen. Die Masse an Daten, die täglich über Wearables wie Smartwatches,  Schrittzähler und Pulsmesser an die Server gesendet werden, bergen schon heute ein enormes Potenzial für Marketingabteilungen von Sportartikelherstellern. Wer fundierte Geschäftsentscheidungen treffen will und wissen möchte, was Kunden wirklich wollen, muss die Daten in seinem Unternehmen analysieren. Dabei gilt es, möglichst viele und unterschiedliche Quellen anzuzapfen, um Daten und Informationen sinnvoll miteinander zu verknüpfen und in Relation zu setzen. Datenwachstum kurbelt Wettbewerb an Der Markt für Anwendungen rund um das Thema Datenanalyse boomt. Gartner prophezeit, dass 2018 rund 50 Prozent der Gewinne vom Datengeschäft verursacht werden. Bis 2020 werden etwa 40 Prozent der Investments, die Unternehmen tätigen, die Bereiche Analytics und Business Intelligence betreffen. Bis 2018 werden sich außerdem 75 Prozent der Technologieunternehmen auf das Thema Datengenerierung und strategie-orientierte Analysieren fokussieren und entsprechende Produkte dazu anbieten. Das kurbelt auch den Wettbewerb an: Laut Gartner werden 2018 über die Hälfte der großen Konzerne nicht nur mit ihren Produkten miteinander in Wettbewerb treten, sondern auch um das Thema Datengewinnung  miteinander konkurrieren.   Suchmaschinen für strukturierte und unstrukturierte Daten Wenn es darum geht, Daten aufzuspüren und diese sinnvoll auszuwerten, stoßen Dokumentenmanagementsysteme oft an ihre Grenzen. Meist sind die Daten über verschiedene Systeme, Wikis oder Verzeichnisse verstreut und lagern sowohl auf Festplatten als auch in der Cloud. Unternehmen brauchen daher eigene Softwarelösungen, um die ständig wachsende Anzahl an strukturierten und unstrukturierten Datenmengen verarbeiten zu können und die aus den Daten gewonnenen Erkenntnisse für sich zu nutzen. Doch zuvor muss man die Daten, die man braucht, erst einmal finden. Nahezu jede Applikation verfügt mittlerweile über eine Suchfunktion, doch die Suche beschränkt sich hier meist nur auf die Bereiche innerhalb der Applikation oder des Dokuments. Um Daten unternehmensweit aufzuspüren, helfen spezielle Suchmaschinen, sogenannte Enterprise Search Anwendungen.  Über ein einziges Interface kann der Mitarbeiter die relevanten Informationen aufspüren,  die er für seine Arbeit braucht. Im Unterschied zu den bekannten Web-Suchmaschinen wie Google durchsuchen Enterprise Search Anwendung nicht nur eine Quelle, wie zum Beispiel das Web. Professionelle Unternehmenssuchmaschinen verfügen über Schnittstellen zu Unternehmensprogrammen wie E-Mail-Anwendungen, Content Management Systemen, Dokumentenmanagementsystemen oder Netzlaufwerken. Egal, ob der Content in der Cloud oder auf den Laufwerken gespeichert ist oder ob die Daten strukturiert oder unstrukturiert vorliegen: Die Suchmaschine holt sich die Informationen über Standardkonnektoren aus den einzelnen Applikationen und listet diese unter Berücksichtigung der Quelle auf. Der Nutzer bekommt so eine 360-Grad-Ansicht auf alle wichtigen Informationen und weiß, ob die Information beispielsweise aus dem CRM-System, dem Mailprogramm oder aus einem Textdokument entstammt.  Die integrierte Rechteverwaltung gewährleistet, dass die Nutzer nur die Daten in der Trefferliste angezeigt bekommen, auf die sie befugt sind, zuzugreifen.  Suche als Grundlage für Analytics Professionelle Enterprise Search Anwendungen können Daten mittlerweile nicht nur aufspüren, sondern dem Nutzer auch bei der Analyse helfen.  Im Gegensatz zu einfachen Suchmaschinen stellen professionelle Enterprise Search Lösungen Zusammenhänge her, wie zum Beispiel zwischen Kundennummer und den dazugehörigen Dokumenten, die für den Kunden hinterlegt worden sind. Auf diese Weise kann der Nutzer sich schnell einen Überblick über ein bestimmtes Thema verschaffen und zum richtigen Zeitpunkt die richtige Entscheidung für sein Unternehmen, sein Produkt oder seine Marke treffen.  Die intelligente Suche mit Content Analytics Funktionen bietet für Unternehmen einen nicht zu unterschätzenden Mehrwert. Gartner prophezeit, dass schon Ende 2017 rund 25 Prozent der Mitarbeiter bis zu fünfmal am Tag mit Suchtechnologien in Berührung kommen werden. Die Suche wird Bestandteil von Business Intelligence und vom Geschäft mit den Daten: Bis 2018, so Gartner, werden sich Enterprise Search Anwendungen in die nächste Generation weiterentwickeln und eine Komponente von Business Intelligence und Analytics Plattformen sein. Die Suche wird damit zur Grundlage für Analytics Anwendungen. Von Enterprise Search zu Insight Engine Gartner spricht in diesem Zusammenhang von einer Entwicklung von „Enterprise Search“ zu „Insight Engine“. Die Insight Engine erlaubt es Unternehmen, eine einheitliche Sicht auf verschiedene Arten von Informationen zu bekommen, egal, ob sie aus der Cloud stammen oder von Third-Party-Applikationen oder aus den Netzwerken. Das erlaubt es, Daten aus ERP und CRM-Systemen gleichermaßen aufzuzeigen und zu kombinieren  ohne sie jedoch durcheinanderzubringen. Die Ursprungsquelle bleibt im Suchindex dabei immer erhalten. Gartner empfiehlt den Anbietern von Enterprise Search Software, die Anwendungen „natural“, „total“ und „proactive“ zu gestalten. Durch Google Now, Yelp, Waze und Siri hat sich die natürlichsprachige Suche insbesondere auf den Smartphones durchgesetzt. Nutzer erwarten von ihrer Enterprise Search Anwendungen, dass sie diese Funktionalitäten ebenso unterstützt. Die semantische Suche wird sich neben der keywordbasierten Suche immer weiter durchsetzen. In zwei Jahren werden laut Gartner fast ein Drittel der Suchanfragen mit Fragewörtern wie „Was“, „Wer“, „Wie“ oder „Wann“ beginnen.  Mit „total“ appelliert Gartner an die Anbieter, möglichst viele Datenquellen anzubinden, damit der Anwender ein ganzheitliches Bild vom Sachverhalt vermittelt bekommt und Zusammenhänge schneller erkennt. Darüber hinaus soll der Enterprise Search Anbieter seine Nutzer „proaktiv“ über relevante und verwandte Themen auf dem Laufenden halten. Eine Suche wird dann nicht mehr notwendig sein. Der Nutzer bekommt alle Informationen, die er für seine Arbeit braucht, direkt auf sein Gerät per Push geschickt.   Der Autor Franz Kögl ist Mitgründer und -inhaber der Firma IntraFind Software AG und verfügt über mehr als 15 Jahre Erfahrung im Enterprise Search und Content Analytics Bereich. Mehr über Franz Kögl finden Sie in unserem Management Profil  </body></Document><Document><url>https://www.intrafind.de/blog/hardware-war-gestern-sehen-wir-schon-immer-so</url><id>F83B60E49363106D3F4A153ABDA6017C</id><title>Hardware war gestern? Stimmt, das sehen wir schon immer so.</title><language>de</language><body>Hardware war gestern? Stimmt, das sehen wir schon immer so. Google kündigt Google Search Appliance (GSA) ab.   Google hat seinen Gold-Partnern kürzlich mitgeteilt, sein seit 2002 auf dem Markt befindliches Hardware- Software Bundle, Google Search Appliance, ab 2017 nicht mehr verkaufen zu wollen. Als Hintergründe wurden genannt, dass all die Komponenten und Funktionalitäten, die Google derzeit seinen B2C- Anwendern über www.google.com bereitstellt, nicht mehr in eine einzelne Hardware integrierbar sind. Die Anforderungen der Anwender sind gewachsen und jeder erwartet, dass die Möglichkeiten, die die klassische Internetsuche bietet, auch in einer unternehmensweiten Suche mit der GSA verfügbar sind. Google möchte dieses Problem mit einer Cloud-basierten Anwendung für Unternehmen weltweit lösen. Was hat es mit der Search-Cloud von Google auf sich? Unsere Idee dazu sieht folgendermaßen aus:   Gerade für die Entwicklung der natürlichsprachigen Suche, wie wir sie aktuell von Apple‘s Siri kennen, ist es Google sehr von Nutzen, massenhaft Suchmuster bzw. Suchverhalten zu speichern, zu analysieren und auszuwerten, um letztendlich daraus „Intelligentes“ und „Verkaufbares“ ableiten zu können. Um diese Entwicklung voranzutreiben, benötigt Google ein enormes weltweites Informationspaket aus sämtlichen Unternehmensumfeldern.   Im klassischen B2C Anwenderbereich kann Google wie auch Facebook auf dieses beachtliche Wissen zurückgreifen, welches sich aus Suchanfragen, Inhalten auf Webseiten, Darstellung und Nutzung der Trefferlisten, Akzeptanz des organischen Rankings, Klickverhalten oder Lesefluss zusammensetzt. Es ergeben sich Abermillionen Profile, die in der Masse mit statistischer Verfahren ausgewertet werden. Das Ergebnis sind beispielsweise Anwendungen wie Google Translate, die mit der Markteinführung und Marktakzeptanz ganze Unternehmen verschwinden ließen. Vielen Anwendern ist diese Entwicklung unheimlich, da für nur sehr wenige einsehbar und kalkulierbar.   Doch ist jedes Unternehmen, welches heute die GSA integriert hat, bereit sein Unternehmenswissen, seine Unternehmensinformationen in der Cloud zu speichern und damit bereit das Risiko einzugehen, dass im worst-case Suchanfragen, Suchverhalten von Mitarbeitern für statistische Auswertungen genutzt werden könnten?   Um noch kurz auf die Hardware-Frage einzugehen. Viele Suchanbieter für eine unternehmensweite Suche setzten und setzen heute noch auf den Google-Trend und investierten in starre Hardware-Lösungen, die dem Kunden nur ganz begrenzt Einblick in Verfahren bieten. Wir von IntraFind sind auch der Meinung, dass eine Blackbox keine Lösung ist, da Skalierbarkeit und Performance damit ihre Grenzen haben. Und wie wir bereits von unseren Kunden wissen, ein hohes Maß an Individualprogrammierungen und Workarounds benötigen. Also, warum sollte man eine von Google als zukunftslos betrachtete Lösung - nämlich eine Search Appliance - durch eine Kopie ersetzen?   Wir interessieren uns für Ihre Haltung zu diesem Thema. 3 kurze Fragen haben wir vorbereitet &gt;&gt;   Der Autor Anke Mittelstädt ist seit 2012 bei IntraFind beschäftigt und verantwortet den Bereich Marketing und Public Relations. Sie blickt derzeit auf über 15 Jahre IT-Marketing Erfahrung zurück. In der Zeit vor IntraFind arbeitete Anke Mittelstädt als Manager Marketing EMEA &amp; APAC bei Iron Mountain Digital. Weitere 5 Jahre verbrachte sie bei dem Storage-Hersteller NetApp. In ihrer Rolle als Enterprise Marketing Manager erarbeitete sie erfolgreiche Marketing-Strategien und -Kampagnen und trug so zum Unternehmenserfolg bei. Die Diplom Betriebswirtin startete ihre berufliche Laufbahn im Marketing und Vertrieb der Infineon Technologies AG. Anke Mittelstädt lebt in München.</body></Document><Document><url>https://www.intrafind.de/blog/sie-wollen-google-nicht-in-die-cloud-folgen-muessen-sie-auch-nicht</url><id>C418F51693A1A5290F8CF51BD628D131</id><title>Sie wollen Google nicht in die Cloud folgen? Müssen Sie auch nicht.</title><language>de</language><body>Sie wollen Google nicht in die Cloud folgen? Müssen Sie auch nicht. IntraFind bietet Ihnen mit dem iFinder5 elastic eine interessante Alternative zur Google Search Appliance (GSA). Wenn eine Verlängerung der GSA in Ihrem Unternehmen ansteht oder Sie kurzfristig auf eine zukunftssichere und stabile Lösung wechseln möchten, ist der folgende Blogbeitrag genau das Richtige für Sie. Viele Kunden investierten in eine GSA aufgrund des Markennamens Google - und nicht, weil eine Appliance gefordert war. Aus Sicht von Google stellt sich die Situation folgendermaßen dar: Mit der Abkündigung wird die letzte verbliebene Nicht-Cloud-Lösung aus dem Portfolio genommen. Eine Weiterentwicklung des Produkts oder auch der Support werden künftig sicher nur noch halbherzig verfolgt. Die GSA kam überwiegend für die Suche auf Webseiten, in Online Shops und in Intranets zum Einsatz - ein Umfeld, in dem eine rechtegeprüfte Suche oft nicht zwingend durchgängig erforderlich ist. Falls Sie Ihre bestehende Lösung ablösen möchten, dann hat IntraFind mit dem iFinder als Virtual Appliance eine zeitgemäße Alternative parat. Installation und Konfiguration nehmen nicht mehr Zeit in Anspruch, die Administration ist übersichtlich und zeitsparend und eine bestehende Content Feed-Architektur für die Datenanlieferung an die GSA kann 1:1 beibehalten werden. Das garantieren wir Ihnen. Wir als deutsches Unternehmen stehen für Datensicherheit und Datenschutz. Der iFinder telefoniert nicht nach Hause und liefert auch deutlich mehr Suchfunktionalität als die GSA. Wir als deutsches Unternehmen liefern Ihnen die besten Ergebnisse oben in der Trefferliste, weil wir viel in unsere Entwicklung investiert haben, um insbesondere deutschsprachige Dokumente optimal verarbeiten zu können. Dies gilt auch für weitere 32 Hauptwirtschaftssprachen. Wir zwingen unsere Kunden nicht, in die Cloud zu gehen. Der iFinder kann sowohl bei Ihnen in Ihrer eigenen IT-Infrastruktur oder auch in der Cloud betrieben werden. Wir richten uns nach Ihnen - und nicht umgekehrt: Sie müssen nicht gemäß den Vorstellungen des Anbieters handeln. Unlimitierte Skalierbarkeit, Lastverteilung und Real-Time-Indexierung garantieren, dass der iFinder immer mit Ihren Anforderungen mitwachsen kann. Wir verwenden Elasticsearch als Basis unserer modernen Such- und Indexlösung. Eine Kombination aus Hard- und Software macht noch lange keine gute, moderne Suche. Suche muss sicher sein, Suche muss einfach administrierbar sein, darauf verlassen sich über 1000 iFinder Kunden seit vielen Jahren. Suche muss aber auch sexy sein... Unsere Autovervollständigung in Kombination mit semantischen Technologien und dem Wissen um die Komplexität der deutschen Sprache begeistert durch eingebaute Tippfehlerkorrektur und sinntragende Erweiterungsvorschläge. Damit liefern wir schon hier den Kontext zu einer Suchanfrage aus den Dokumenteninhalten und helfen so, schnell das eine gesuchte Dokument zu finden oder unterstützen beim Stöbern in unbekannten Inhalten. Und dabei sind alle Informationen rechtegeprüft im Kontext des angemeldeten Benutzers. Bei der Suche in Shops werden z.B. alle gesetzten Suchfilter auch auf das Autocomplete angewendet. Sie sehen: rundherum eine echte und dazu noch preiswertere Alternative zu Ihrer GSA! Unser Angebot Schauen Sie sich den iFinder im Rahmen einer Websession oder eines aufgezeichneten Webcasts an. Wir besprechen Ihre individuelle Situation und erstellen für Sie ein maßgeschneidertes, individuelles Angebot. Und wenn wir wie so oft schon die GSA abgelöst haben, kümmern wir uns auch um die kostenfreie Entsorgung Ihres Elektroschrotts. Wir freuen uns auf Ihre Anfrage. Webcast vom 12. Mai 2016 (Aufzeichnung) Mehr Informationen zu diesem Thema gibt es in diesem Webcast: &quot;Sie suchen Ersatz für Ihre Google Search Appliance?&quot;   Der Autor Franz Kögl ist Mitgründer und -inhaber der Firma IntraFind Software AG und verfügt über mehr als 15 Jahre Erfahrung im Enterprise Search und Content Analytics Bereich. Mehr über Franz Kögl finden Sie in unserem Management Profil  </body></Document><Document><url>https://www.intrafind.de/blog/suche-big-data-eventual-consistency-warum-suche-in-extrem-grossen-datenmengen-nicht-mehr-zwingend-konsistent-sein-kann</url><id>AF178F1BB5E9771497F36929903EF57E</id><title>Suche   Big Data = Eventual Consistency. Teil 1 - Warum Suche in extrem großen Datenmengen nicht mehr zwingend konsistent sein kann</title><language>de</language><body>Suche + Big Data = Eventual Consistency. Teil 1 - Warum Suche in extrem großen Datenmengen nicht mehr zwingend konsistent sein kann „640 kB sollten eigentlich genug für jeden sein.“ Viele von uns kennen vermutlich diesen Spruch, den Bill Gates angeblich im Jahr 1981 gemacht haben soll. Heute misst man den Arbeitsspeicher eines Rechners in Gigabytes und würde man sich zu der Aussage hinreißen lassen, dass z.B. 8 Gigabyte Hauptspeicher doch sicherlich für alle Zeiten ausreichend sein sollten, könnte man sich sicher sein, dass auch diese Behauptung in einigen Jahren skurril wirken würde. Die weltweite Datenmenge wächst, unaufhaltsam und immer schneller. Der digitale Fußabdruck, den jeder von uns Jahr für Jahr hinterlässt, wird immer größer. Die gesamte Datenmenge, die die Menschheit vom Anbeginn der Geschichtsschreibung bis in das Jahr 1986 erzeugt hat, beträgt etwa 2.6 Exabytes (= 300.000x die größte Bibliothek dieser Welt). Im Jahr 2016 entsteht jeden Tag eine neue Datenmenge dieser Größe. Diese Entwicklung ist exponentiell, das digitale Datenvolumen verdoppelt sich etwa alle zwei Jahre - mit Konsequenzen für die Verarbeitung dieser Datenflut. War es früher noch möglich, die in einem Unternehmen entstehenden Daten mit einem einzigen Rechner zu verarbeiten, ist dies heute in vielen Fällen nicht mehr denkbar. Zwar gilt (mit gewissen Einschränkungen) noch immer das Mooresche Gesetz, demzufolge sich die Komplexität integrierter Schaltkreise alle 24 Monate verdoppelt. Allerdings steigt die effektive Geschwindigkeit, mit der Daten verarbeitet werden können, nicht linear mit. Eine Revolution stellt die Solid State Disc da, die durch ihre extrem niedrigen Zugriffszeiten Verfahren ermöglicht, die lange Zeit nicht praktikabel waren. Doch gibt es Grenzen für die Leistungsfähigkeit eines einzelnen Rechners – physikalisch oder finanziell (vertikale Skalierung war schon immer teuer und wenig kosteneffektiv). Damit bleibt irgendwann nur noch die horizontale Skalierung als Lösungsansatz übrig: statt eines sehr teuren monolithischen Hochleistungsrechners setzt man auf einen Verbund vieler günstiger Rechner. In Summe erhält man so erheblich mehr Rechenleistung für das gleiche Geld. Wir folgern: Eine Big Data-fähige Technologie ist zwingend verteilt. Unser verteiltes System ist Big Data-fähig, kostengünstig und schnell. Damit ist doch eigentlich alles geklärt… oder etwa doch nicht? Dazu muss man wissen, dass für verteilte Systeme folgendes Gesetz gilt: CAP Theorem Ein verteiltes System kann maximal zwei der folgenden drei Eigenschaften aufweisen: (C) Consistent/Konsistent: Jeder Knoten sieht zu jedem Zeitpunkt die gleichen Daten. (A) Available/Verfügbar: Jede Anfrage an das System wird zeitnah beantwortet. (P) Partition tolerant/Partitionstolerant: Das System arbeitet auch bei Knoten- oder Kommunikationsausfällen weiter. Anschaulich wird dies anhand eines Beispiels: Über zwei Reisebüros A, B wird das letzte Ticket für einen Flug angeboten. Die beiden Standorte koordinieren den Verkauf über eine Telefonleitung, um sicherzustellen, dass der Platz nicht doppelt vergeben wird. Büro A wird nun von einem Kunden 1 betreten, der die Reise buchen möchte. Der Verkäufer aus A ruft kurz in Büro B an und informiert darüber, dass die Reise nun verkauft wird. Der Kunde verlässt A mit seinem exklusiven Ticket. Nun betritt ein Kunde 2 Büro B, möchte ebenfalls die Reise buchen, wird aber vom Verkäufer dort darüber informiert, dass der Flug ausgebucht ist. In diesem Szenario hat alles geklappt: Die Kunden mussten nicht warten (Bild 2 und 4), und die Reise wurde exakt 1x verkauft (Bild 3). Aber es gab ja auch keinen Netzwerkausfall! Nun betrachten wir, was passiert, wenn die Telefonleitung ausgefallen ist: Kunde 1 möchte die Reise buchen, aber der Verkäufer von A kann Büro B nicht erreichen. Er hat nun drei Möglichkeiten: 1. Warten, bis die Telefonleitung wieder steht. Damit ist das System nicht mehr verfügbar. 2. Das Ticket nicht verkaufen. Das wäre eine Form der Konsistenzverletzung, denn tatsächlich existiert ja ein „verkaufsfähiges“ Ticket. 3. Das Ticket verkaufen und hoffen, dass es nicht gleichzeitig in Büro B verkauft wird (was ein eindeutiger Bruch der Konsistenz wäre). Dass das CAP-Theorem für eine verteilte Architektur immanent ist, bedeutet aber nicht, dass cleveres Vorgehen die Auswirkung in der Praxis nicht doch verbessern kann. So kann man in unserem Beispielcluster das Verhalten deutlich verbessern, wenn Reisebüro A das Ticket auch dann verkaufen darf, wenn B nicht erreichbar ist, wohingegen B in diesem Fall warten oder den Verkauf verweigern muss (A wird also eine Art privilegierter Knoten).Wir haben die konsistente Verfügbarkeit trotz komplettem Netzwerksausfall von 0% auf 50% gesteigert! Dennoch gilt folgende Aussage: Eine Big Data-fähige (und damit verteilte) Technologie (z.B. unternehmensweite Suche oder Content Analytics) muss zwangsläufig eine der Eigenschaften konsistent, verfügbar, partitionstolerant aufgeben. Was bedeutet das für eine Suchmaschine bzw. für Enterprise Search? Oder anders formuliert: Worauf können wir verzichten, und welche Auswirkungen hat dies in der Praxis? Antworten darauf liefert Ihnen in Kürze unser nächster Blogartikel. Der Autor Jörg Viechtbauer ist seit Oktober 2012 als Software Architekt bei der IntraFind Software AG beschäftigt. Nach seinem Informatikstudium an der RWTH Aachen war Jörg Viechtbauer in der Softwareentwicklung namhafter deutscher Softwareunternehmen tätig und sammelte umfassende Praxiserfahrung in der Konzeption und Implementierung von Enterprise Search-Lösungen. Aktuell gilt sein Interesse besonders der Entwicklung von Suchlösungen, Services &amp; Plugins auf Basis von Elasticsearch.</body></Document><Document><url>https://www.intrafind.de/blog/die-suche-nach-der-perfekten-suche-im-intranet</url><id>C4959084B8952EBF4F87ACFCA7B810FD</id><title>Die Suche nach der perfekten Suche im Intranet</title><language>de</language><body>Die Suche nach der perfekten Suche im Intranet   In meiner langjährigen Tätigkeit als IT-Berater, der sich mit Technologien und Lösungen für Wissens-Arbeiter oder neudeutsch &quot;Knowledge Worker&quot; beschäftigt, musste ich immer wieder feststellen, dass das Thema „Suche im Unternehmen“ - über alle Branchen und Unternehmensgrößen hinweg - eher stiefmütterlich behandelt wird.   Ich kenne wenige Unternehmen, in denen „Suche“ ein klar definierter Teil der Unternehmens- und IT-Strategie ist.   Diese Tatsache ist für mich verwunderlich, besonders in Anbetracht dessen, dass ich mich beruflich lange Zeit in den Themenfeldern Collaboration, Knowledge Management und Dokumentenmanagement bewegte. Hierfür kommen in der Regel in Unternehmen sehr heterogene Systeme zum Einsatz, die durchaus nicht einfach miteinander gekoppelt werden können. Aber gerade im Umfeld von Systemen, die Daten verwalten und für Anwender bereitstellen, sind Recherchen Alltag und müssen im Unternehmensalltag häufig über verschiedenste Systeme gemacht werden, um ein vollständiges Bild zu einem Sachverhalt oder Antworten auf aktuelle Fragestellungen zu erhalten.   Natürlich bieten die genannten Systeme inzwischen in der Regel eine Möglichkeit zum Auffinden von Inhalten an, aber eben meist nur in Form einer applikationsspezifischen Suche innerhalb des Systems selbst.   Als Folge daraus wird eine Recherche über mehrere Systeme hinweg zu einem zeit- und nervenraubenden Akt. Dieser resultiert oft auch noch darin, dass der geplagte Mitarbeiter die gefundenen Ergebnisse aus den verschiedenen Systemen - mit allen Medienbrüchen behaftet - häufig noch manuell zu einem Ergebnis zusammenfassen muss. Dies ist übrigens einer der Hauptgründe, warum sich so viele Dateien in unzähligen Kopien auf lokalen Festplatten, Benutzerlaufwerken, in E-Mail-Ordnern etc. wiederfinden.   Die geschilderte Problematik war Motivation genug für mich, Ihnen mit dieser neuen Blogreihe das Thema „Suche“ näher zu bringen und den einen oder anderen Aspekt genauer zu beleuchten, der Augenmerk verdient, wenn man sich mit dem Thema „Suchen und Finden von Informationen“ auseinander setzt.   Das Für und Wider einer unternehmensweiten Suche   Wenn ich mit meinen Kunden diskutiere, warum Suchprojekte nur zögerlich in Angriff genommen werden, höre ich die unterschiedlichsten Gründe.   Die Ansicht, dass die Mitarbeiter „eine Suche brauchen könnten&quot;, teilen fast alle.   Oft steht die Befürchtung im Raum, dass die Einführung einer Unternehmenssuche eine anspruchsvolle Aufgabe sei, vergleichbar mit der Einführung eines Intranet-Portals.   Und in vielen Unternehmen ist den verantwortlichen Entscheidern der echte Mehrwert einer hochwertigen Unternehmenssuche nicht wirklich klar. Der &quot;Return of Investment (ROI)&quot; lässt sich schwer vorab beziffern und damit wird die Investition in die Anschaffung einer Enterprise Search Software sowie in die Projekte gescheut.   Und hier nähern wir uns aus meiner Sicht dem entscheidenden Punkt.   Für mich steht außer Frage, dass mit Suche eine Menge Geld eingespart und verdient werden kann. Sehen Sie sich beispielsweise nur die kommerziellen, frei zugänglichen Medienportale und Handelsplattformen im Internet an: die guten basieren ausnahmslos alle auf Suche.   Im Unternehmensumfeld kommt es hingegen auf den konkreten Anwendungsfall oder neudeutsch &quot;Use Case&quot; an, für den eine ROI-Betrachtung gemacht wird - und der ist nicht immer sofort ersichtlich. Ich werde Ihnen aber im Verlauf dieser Blogreihe immer wieder Anregungen für mögliche Use Cases für Suche in Ihrem Unternehmen geben.   Das Bild des EINEN Intranet-Portals als „single point of information“, das viele im Kopf haben und auf die Suche übertragen, stellt eine gute Ausgangsbasis dar.   Denn wenn eine unternehmensweite Suche in dem EINEN Suchportal mündet oder in das Intranet-Portal des Unternehmens integriert wird, findet sich der Benutzer bestätigt, dass er über das Portal alles findet, was er für sein (berufliches) Leben braucht. Er wird es als wichtiges Recherchewerkzeug nutzen und dies führt zu einer nachhaltigen Anwenderakzeptanz.   Natürlich hat eine unternehmensweite Suche über die Portal-Suche hinaus weitaus mehr Facetten, die in vielen Fällen sogar für Benutzer völlig transparent sind und daher die Argumentation für die Einführung einer unternehmensweiten Suche stützen. Es gibt unzählige Möglichkeiten, Suche im Unternehmen zu platzieren und sie in Prozesse oder Anwendungen zu integrieren. Ein Kunde aus der Medien- und Verlagsbranche beispielsweise nutzt Suche als automatisierte Redaktionsunterstützung in einem Content-Management-System: durch das Auffinden von inhaltlich ähnlichen Artikeln zum geschriebenen Text wird verhindert, dass Artikel zu demselben Sachverhalt doppelt erstellt und veröffentlicht werden.   Die Vorteile von Suche im Unternehmen sind vielfältig: wertvolle Zeitersparnis, mehr Komfort und Effizienz für die Benutzer, aber auch eine höhere Qualität und Nutzbarkeit neu erstellter sowie historischer Unternehmensdaten. Entscheidend ist, dass mit einer guten Suchmaschine all diese Anwendungsszenarien realisiert werden können, da die Suche überall auf die gleiche Art und Weise funktioniert.   Im nächsten Beitrag erfahren Sie, was es bei der Planung und Umsetzung von Enterprise Search-Projekten zu beachten gibt.   Der Autor Patrick Baldi verfügt über mehr als 20 Jahre Erfahrung als Berater in der IT-Branche. Lange Jahre war er im Consulting und technischen Vertrieb der Microsoft Deutschland GmbH tätig, wo er schwerpunktmäßig Kunden rund um die Themen Architekturen und Anwendungen im Bereich Collaboration, Intranet, Knowledge Management und Suche betreute. &quot;Meine Leidenschaft für das Thema Suche wuchs aus der Erkenntnis, dass das Potential und die Möglichkeiten von Suche im Unternehmensumfeld immer noch weitgehend verkannt werden.&quot;</body></Document><Document><url>https://www.intrafind.de/blog/suche-big-data-eventual-consistency-teil-2-warum-suche-in-extrem-grossen-datenmengen-nicht-mehr-zwingend-konsistent-sein-kann</url><id>8EA44AAE6B11844BB5A0B5FF75B6DD46</id><title>Suche   Big Data = Eventual Consistency Teil 2 - Warum Suche in extrem großen Datenmengen nicht mehr zwingend konsistent sein kann</title><language>de</language><body>Suche + Big Data = Eventual Consistency Teil 2 - Warum Suche in extrem großen Datenmengen nicht mehr zwingend konsistent sein kann Fortsetzung Blogbeitrag von Jörg Viechtbauer vom 19.05.2015 Der erste Teil des letzten Blogbeitrages endete mit der Feststellung: Eine Big Data-fähige (und damit verteilte) Technologie muss zwangsläufig eine der Eigenschaften konsistent, verfügbar, partitionstolerant aufgeben. Teil 2 soll nun Antworten auf folgende Fragen liefern: Was bedeutet das für eine Suchmaschine? Oder anders formuliert: Worauf können wir verzichten, und welche Auswirkungen hat dies in der Praxis? Die Partitionstoleranz ist kaum verhandelbar, denn sie wird umso wichtiger, je höher man skalieren möchte. In einem Verbund von 300 Rechnern ist zu erwarten, dass im Schnitt jede Woche mindestens eine Maschine ausfällt. Wie wichtig ist die Verfügbarkeit? 2005 wurde bei Google die Trefferliste experimentell auf 30 Einträge vergrößert – mit dem Ergebnis, dass der Traffic um 20% einbrach. Als Ursache wurde der Anstieg der Suchzeit von 400 auf 900 Millisekunden identifiziert. Einmal mehr gilt: Zeit ist Geld! Selbst eine geringfügige Verschlechterung der Verfügbarkeit hätte Google Milliarden gekostet. Somit sind Abstriche bei der Verfügbarkeit nicht akzeptabel. Bleibt also unabwendbar nur noch die Konsistenz als verzichtbare Eigenschaft. Verlust der Konsistenz – das klingt beunruhigender als es ist, betrifft die Inkonsistenz doch lediglich die während des Netzwerkausfalls aufgelaufenen Operationen und somit nur einen Bruchteil der Gesamtdatenmenge. Was bedeutet der Verlust von Konsistenz für Suche / Enterprise Search? Bei einer Suchmaschine kann sich dies z.B. so bemerkbar machen, dass neu indizierte Dokumente erst zeitverzögert gefunden werden können oder – wenn das System beispielsweise durch Replikation hochverfügbar (und damit noch verteilter) gemacht wird – temporär alternierende Ergebnislisten (mal wird das neue Dokument gefunden und dann wieder nicht). Vielleicht sind auch die Facettenwerte nicht exakt oder das Dokument wird zunächst nicht präzise gerankt… die konkreten Auswirkungen hängen von der Implementierung ab, sind bei einer Suchmaschine aber in den allermeisten Fällen kaum wahrnehmbar und stellen daher keine Einschränkung der Funktionalität dar. Oder haben Sie schon einmal bemerkt, dass die Google Suche inkonsistent ist? Und wenn ja, hat es Sie gestört? Wenn die unmittelbare Konsistenz schon nicht vollständig garantiert werden kann, gibt es in den meisten Architekturen doch eine gewisse Annäherung daran, die mit dem englischen Begriff eventual consistency bezeichnet wird. Ein System heißt eventual consistent, wenn es garantiert, dass es zu einem konsistenten Zustand konvergiert, d.h. irgendwann einmal konsistent wird. Auch hier kann man keine pauschalen Aussagen darüber treffen, was dies konkret bedeutet und wie lange es dauern wird, bis dieser Zustand erreicht ist. In der Regel wird das System nach Wiederherstellung der Konnektivität die Konsistenz überprüfen und gegebenenfalls mit einer Art Reparatur beginnen. Sollen in unserem Beispiel die Reisebüros verfügbar und partitionstolerant sein, verkaufen die Verkäufer das Ticket auch dann, wenn die Leitung zusammengebrochen ist, versuchen danach aber jede Minute, den Kollegen zu erreichen. Sobald dies gelingt, wird der Konsistenzzustand überprüft: Wurde das Ticket nicht oder nur einmal verkauft, ist alles in Ordnung. Wurde das Ticket hingegen fälschlicherweise doppelt verkauft, erfolgt nun die Wiederherstellung eines gültigen Zustands. Ein Kunde wird über den Fehler unterrichtet, erhält eine Rückerstattung und als Kompensation z.B. eine gute Flasche Wein. FAZIT Die digitale Datenflut wächst exponentiell und schneller als sich die Rechenleistung eines einzelnen Rechners entwickelt. Eine Big Data-fähige Architektur muss daher zwangsläufig horizontal skalieren, Somit unterliegt sie dem CAP-Theorem, demzufolge ein verteiltes System nicht gleichzeitig sowohl konsistent, verfügbar als auch partitionstolerant sein kann. Bei einer Suchmaschine wird die Konsistenz – als der am wenigsten ins Gewicht fallende Faktor – durch eine schwächeren Konsistenzgarantie ersetzt: der eventual consistency. Diese gewährleistet, dass das System – so schon nicht unmittelbar – doch irgendwann einmal konsistent sein wird. Der Autor Jörg Viechtbauer ist seit Oktober 2012 als Software Architekt bei der IntraFind Software AG beschäftigt. Nach seinem Informatikstudium an der RWTH Aachen war Jörg Viechtbauer in der Softwareentwicklung namhafter deutscher Softwareunternehmen tätig und sammelte umfassende Praxiserfahrung in der Konzeption und Implementierung von Enterprise Search-Lösungen. Aktuell gilt sein Interesse besonders der Entwicklung von Suchlösungen, Services &amp; Plugins auf Basis von Elasticsearch.</body></Document><Document><url>https://www.intrafind.de/blog/cognitive-computing-alter-wein-in-neuen-schlaeuchen-ein-plaedoyer-fuer-den-boykott-staendig-neuer-it-buzzwords</url><id>F8904B988805A5A270A68854B2F745F6</id><title>&quot;Cognitive Computing&quot; - alter Wein in neuen Schläuchen? Ein Plädoyer für den Boykott ständig neuer IT-Buzzwords.</title><language>de</language><body>&quot;Cognitive Computing&quot; - alter Wein in neuen Schläuchen? Ein Plädoyer für den Boykott ständig neuer IT-Buzzwords.   Eine Bitte an alle IBM Buzzword-Marketeers:   Auf der diesjährigen Knowtech wurde unter anderem in einer Blog-Parade das Thema &quot;Zukunft des Wissensmanagements: Was ändert sich mit Cognitive Computing?&quot; diskutiert. Mir geht das ständige Generieren neuer Hype-Begriffe in der IT ziemlich auf die Nerven.   Ich bin auch davon überzeugt, dass wir mit großen Datenmengen und Verfahren aus den Bereichen Artificial Intelligence (AI), Machine Learning, Statistical Analysis und Text Mining in den nächsten Jahren sehr nützliche und neue Anwendungen realisieren können. Diese Verfahren in die Praxis zu bringen, daran arbeiten wir bei IntraFind schon seit über 10 Jahren. Aber ich sehe eher eine Evolution an Stelle der mit immer neuen Hype-Begriffen propagierten Revolutionen, die mit ihren übertriebenen Versprechungen nur unrealistische Erwartungen bei Unternehmen und Anwendern wecken.   Die von IBM bei Watson eingesetzten Verfahren sind keineswegs so revolutionär wie oft behauptet. Als Erklärung des Begriffs „Cognitive Computing“ finde ich mit Google als ersten deutschsprachigen Treffer z.B. folgende Textpassagen: &quot;Der hier entscheidende technologische Aspekt ist, dass Watson nicht wie bisher mit Nullen und Einsen Ergebnisse nach dem Prinzip &quot;wahr/falsch&quot; beziehungsweise &quot;Transaktion durchgeführt&quot; oder &quot;fehlgeschlagen&quot; ausführt. Vielmehr wird, zwar immer noch mit Hilfe des dualen Systems, auf Grundlage teilweise unsicherer Informationen, Annahmen und auch Spekulationen basierend auf Wahrscheinlichkeiten nach Antworten auf komplexe, in natürlicher Sprache gestellte Fragen gesucht.&quot;   Genau darum geht es doch seit über 50 Jahren in allen Arbeiten im Bereich AI! Watson ist ein tolles AI-System mit beeindruckender Leistung für Jeopardy. Das meiste, was jetzt von IBM unter dem Namen Watson beworben wird, hat aber mit Watson nichts zu tun.    Also bitte, liebe IBM, nicht schon wieder einen neuen Hype um Watson mit dem sinnlosen Begriff &quot;Cognitive Computing&quot;. Mich nervt schon das viele Gerede von &quot;Big Data&quot; ...   Der Autor Dr. Christoph Goller verfügt über 15 Jahre Erfahrung im Enterprise Search Markt. Er promovierte in Computerwissenschaften an der Technischen Universität in München und arbeitete im Anschluss in einigen wissenschaftlichen Projekten zu Themenbereich Künstliche Intelligenz, Machine Learning sowie Neuronale Netze.   Seit 2002 leitet er den Bereich Forschung bei IntraFind und verantwortet IntraFind&apos;s Kern-Suche- und Content Analyse Technologien. Zwischen 2003 und 2007 arbeitete Christoph aktiv als Committer in Apache&apos;s Open Source Projekt Lucene.    </body></Document><Document><url>https://www.intrafind.de/Abteilungsuebergreifender_Nutzen</url><id>3029DEA6D376BA5CA763BDBE592625FC</id><title>Nutzenargumente für Abteilungen</title><language>de</language><body>Abteilungsübergreifender Nutzen IntraFind setzt mit dem Facelift der Benutzeroberfläche neue Standards. Als Mitarbeiter profitieren Sie schon beim Start der webbasierten Oberfläche - mit dem Dashboard. Lesen Sie aktuellste Nachrichten von Ihrem Unternehmen, erfahren Sie Neues aus Ihrem Blog. Das Widget-basierte Dashboard gibt Ihnen viel Raum für kleine personalisierte Alltagshelfer. Teilen Sie Ihre Suchergebnisse mit Ihren Kollegen, lassen Sie sich aktiv Informieren, wenn neue Informationen zu einem Thema abgelegt wurden oder speichern Sie Ihre Suchanfragen bzw. Ihren Rechercheprozess, so dass Sie ihn zu gegebener Zeit wieder aufnehmen können. IntraFind lässt Sie auch mit den Suchergebnissen nicht allein. Schnell, einfach und komfortabel erkennen Sie Dateitypen auf einen Blick. Jeder Dateityp hat seine einzigartige Darstellung. PowerPoint-Präsentationen sind beispielsweise als einzelne Folien dargestellt und können direkt aufgerufen werden. Die universelle Suche über viele Datenquellen (z.B. Dateisystem, Microsoft Exchange, Lotus Notes, Microsoft SharePoint, Confluence) hinweg ermöglicht einen umfassenden und zielgerichteten Einblick in Ihr gewünschtes Informationsmaterial.   &gt;&gt; zurück zu den Unternehmensbereichen</body></Document><Document><url>https://www.intrafind.de/blog/die-datenschutz-grundverordnung-anforderungen-fuer-insight-engines</url><id>38DF7C68BAD6B5DB0D83E5F33B6AD6A2</id><title>Die Datenschutz Grundverordnung – Anforderungen für Insight Engines</title><language>de</language><body>Die Datenschutz Grundverordnung – Anforderungen für Insight Engines Die Datenschutz Grundverordnung – Anforderungen für Insight Engines Ab Mai 2018 tritt die neue DSGVO im Rahmen der europaweiten Datenschutzreform in Kraft. Sie betrifft alle Unternehmen und Dienste, die im Rechtsraum der Europäischen Union angeboten werden. Insbesondere sind aber solche Unternehmen im Fokus der DSGVO, deren Firmensitz nicht im Gebiet der EU liegt und daher die bereits vorhandene und eher strengere Datenschutzanforderung nicht berücksichtigt haben. Die DSGVO garantiert dem Bürger neue weitreichende Rechte. Insbesondere das „Recht auf Vergessen“ wurde neu eingeführt. Auf Anforderung muss das Unternehmen alle personenbezogenen Daten unwiderruflich löschen. Darunter fallen tatsächlich alle Daten, die im Zusammenhang mit einem Benutzer angefallen sind, etwa besuchte Webseiten oder aber herkömmliche personenbezogene Daten, wie Adresse und Telefonnummern. Besteht die eigentliche Zielgruppe der DSGVO eher aus Social Media Diensten, so trifft sie dennoch alle Unternehmen gleichermaßen. Schaut man in die einschlägigen Foren, so schießen bereits diverse „neue“ Lösungen in den Markt, die sich mit den Herausforderungen der DSGVO beschäftigen. Allen ist gemein, dass die Lösungen hinreichend unspezifisch sind. Woran liegt das? Das Recht auf Vergessen Bleiben wir bei der Anforderung des „Recht auf Vergessen“. Werden im Rahmen eines B2C-Geschäfts Daten des Kunden/Käufers aufgenommen, so fallen diese Daten aller Voraussicht nach nicht unter das Recht auf Vergessen, da andere Regelungen dieses Recht einschränken. Hier seien zum Beispiel das BGB, das Handelsrecht und die daraus resultierenden Aufbewahrungsfristen genannt. Sind im selben Unternehmen allerdings Newsletter, Chats und Foren im Einsatz, dann greift wiederum das Recht auf Vergessen. Auf Anforderung sind diese Informationen zu löschen. Unternehmen, die nicht einen extrem perfekten Prozess befolgen, werden sich hier intensiver mit der DSGVO beschäftigen müssen. In der DSGVO sind viele Einschränkungen und Ausnahmemöglichkeiten genannt. Ist das Löschen zum Beispiel wirtschaftlich nicht möglich, technisch nicht umsatzbar, dann gibt es eine Freikarte. Gleichzeitig gilt aber auch der Grundsatz: Sollen heißt Müssen, wenn Du kannst. Also ist das Berufen auf die o.g. Ausnahmen eher keine gute Strategie. Wie der iFinder5 elastic bei der Recherche hilft Damit man weiß, was alles gelöscht werden muss, benötigt man einen Überblick über die gespeicherten Daten eines Anwenders. Bestandssysteme mit Benutzerverwaltung sollten da alle Anforderungen erfüllen können. Speziell wird es, wenn außerhalb der Bestandssysteme Daten verteilt werden, etwa eine eingehende Interessentenanfrage via Email-Kopie zur weiteren Klärung mit den Details des Anwenders weitergeleitet wird. In diesem Fall hilft die eingesetzte Enterprise Search Anwendung wie der iFinder5 elastic. Als Insight Engine ist der iFinder5 elastic die zentrale Anwendung für die unternehmensweite Suche in strukturierten und unstrukturierten Daten. Über ein zentrales, einfach zu bedienendes User Interface kann der Anwender nach entsprechenden Namen der Personen suchen. Der iFinder5 elastic verfügt über zahlreiche Schnittstellen, über die sämtliche Quellen wie Email-Programme und Webportale angebunden werden können. Der Anwender muss bei der Suche nicht zwingend wissen, in welcher Datenquelle die Information liegt. Mit dem iFinder5 elastic können auch „Ausreißer“ wie etwa die weitergeleitete Email mit den Details schnell und zuverlässig identifiziert und dann auch entsprechend aus dem Email-System entfernt werden. Der Autor Ralf Klinkhammer ist seit fast 30 Jahren im Bereich des Enterprise Information Managements, national und international, tätig. Ralf verantwortet seit 2014 das Produkt Management der IntraFind Software AG.  </body></Document><Document><url>https://www.intrafind.de/unternehmen/glossar/lucene.net</url><id>40599B10F9383C3915113516B3EFDBB5</id><title>Lucene.NET</title><language>de</language><body>Lucene.NET   Die Open Source-Technologie Lucene.NET ist ein Apache Projekt auf der Basis des .NET Frameworks, geschrieben in C#.   IntraFind nutzt Lucene als Basis für die Enterprise Search-Lösung iFinder und bietet eine leistungsfähige unternehmensweite Suchmaschine, die alle vorhandenen Applikationen in den Suchprozess mit integrieren kann. zurück  </body></Document><Document><url>https://www.intrafind.de/services/schulungen</url><id>36AD57AF43B7E41446CAF37EDB1211C1</id><title>Schulungsportfolio der IntraFind Software AG</title><language>de</language><body>IntraFind Schulungen – Know-how für Ihr Unternehmen und Ihre Mitarbeiter IntraFind Experten stellen ein umfangreiches Schulungsangebot in den Bereichen Projektmanagement, Suchtechnologien auf Basis von Elasticsearch und Verfahren der Textanalyse bereit und vermitteln den Teilnehmern kompaktes Wissen. Lernen Sie durch den Besuch unserer Schulungen, Enterprise Search zur Produktivitätssteigerung in Ihrem Unternehmen nutzbar zu machen: wir verschaffen Ihnen einen Wissensvorsprung und unterstützen Sie dabei, Ihre Suchlösung optimal zu konfigurieren, zu verwalten und einzusetzen. Die Schulungen finden bei IntraFind in München oder flexibel im Rahmen eines Inhouse-Workshops in Ihrem Unternehmen statt. Das Schulungsportfolio: Erfolgreich Enterprise Search-Projekte umsetzen - aber wie? Verfahren der Textanalyse - Mehrwert aus Textinformationen generieren Expertenwissen rund um iFinder5 elastic Enterprise Search</body></Document><Document><url>https://www.intrafind.de/blog/verborgene-schaetze-finden-bestehende-probleme-aufzeigen-mehrwert-schaffen-durch-fileshareanalyse</url><id>2BE71F4F2261BF84DCBC4B2F428245E5</id><title>Verborgene Schätze finden, bestehende Probleme aufzeigen: Mehrwert schaffen durch Fileshareanalyse</title><language>de</language><body>Verborgene Schätze finden, bestehende Probleme aufzeigen: Mehrwert schaffen durch Fileshareanalyse Verborgene Schätze finden, bestehende Probleme aufzeigen: Mehrwert schaffen durch Fileshareanalyse Ob Handbücher, Datenblätter oder Präsentationen – in vielen Unternehmen sammeln sich auf den Servern immer mehr Daten an. Nicht immer ist die Qualität der Metadaten dieser Dokumente wünschenswert und oft gibt es auch Brüche bei den Zugriffsrechten auf die Daten. Die Realität in vielen Unternehmen, besonders bei der Nutzung von Fileshares, ist die, dass oftmals Dubletten oder veraltete Dokumente den Fileshare „zumüllen“. In der Regel müssen Mitarbeiter jedoch immer genau wissen, wo ein bestimmtes Dokument gespeichert ist und wie es heißt. Wird die Datei jedoch an einem anderen Ort gespeichert oder nicht korrekt bezeichnet oder mit entsprechenden Metadaten versehen, wird das Dokument unter Umständen gar nicht gefunden. Dadurch geht wertvolles Wissen verloren oder das Rad wird zu oft zweimal erfunden, weil dem Mitarbeiter zum Beispiel nicht bewusst ist, dass es eine Information zu einem bestimmten Thema bereits gibt. Eine zusätzliche Herausforderung ergibt sich dann, wenn die Daten auf eine andere IT-Architektur migriert werden müssen. Oftmals steht der IT-Administrator vor der Herausforderung, nur die Daten zu migrieren, die auch wirklich benötigt werden. Dubletten sollten hierbei entfernt werden, so dass Dateien nicht unnötig doppelt umgezogen werden. Das bestehende Rechte- und Rollenkonzept muss dabei jedoch genau betrachtet und auf der Zielplattform eingehalten werden. Die Fileshareanalyse Um herauszufinden, welche Daten migriert werden sollen und welche gelöscht werden können, muss die IT-Abteilung jedoch erst einmal wissen, welche Daten es gibt und wo sich diese befinden. Durch eine tiefgehende Analyse der Fileshares können alle Daten erfasst werden. Dadurch können vor allem drei wichtige Erkenntnisse gewonnen werden: Das Erkennen von Brüchen im Rechtekonzept: Durch Ausschneiden und Einfügen von Ordnern mit vielen angehängten Dateien – ein durchaus gängiges Mittel bei der Migration von Abteilungsdaten – kommt es oft zu Rechtebrüchen im Fileshare. Das Problem, das sich daraus ergibt, ist, dass Ordner, auf denen Berechtigungen fehlen, im Fileexplorer nicht angezeigt werden. Dokumente, die aber unterhalb der Ordner liegen, besitzen oftmals wieder die Rechte des Benutzers. Ein Problem bei Migrationsprojekten oder auch bei einem Enterprise-Search-Projekt ist, dass nun die Suchmaschine diese Dokumente richtig anhand der bestehenden Dateirechten in der Trefferliste anzeigt, obwohl der Ordner gegebenenfalls nicht im Fileexplorer zu sehen ist. Die Sicht auf Metadaten. Hier lässt sich unter anderem erkennen, wie oft auf die Daten zugegriffen wird und welche Dateiformate wie häufig in welchen Bereichen vorkommen. Auch das Alter der Inhalte kann analysiert werden. Die inhaltliche Analyse. Hier geht es darum, mit Content Analytics Verfahren die wesentlichsten Inhalte zu extrahieren. In der Analyse werden auch die Verzeichnispfade auf Top-Themen und wichtige Metadaten hin analysiert, innerhalb der Dokumente werden wichtige Schlagworte erkannt, sowie Informationen über Produkte, Mitarbeiter bzw. Personennamen, Orte oder Firmennamen gesammelt und in der Analyse-Plattform zur Anzeige gebracht. Der Nutzen Durch die beschriebene Detail-Analyse der Filesharedaten kann ein Securitymanager Vertrauen schaffen, indem die Daten sauber migriert werden und die Sicherheitsanforderungen, wie integrierte Rechtekonzepte, eingehalten werden. Ebenso wichtig ist zudem der Aspekt der inhaltlichen Analyse. Welche Dokumente müssten gemäß gesetzlicher oder unternehmensspezifischer Löschfristen und Compliance-Richtlinien aus den Speichersystemen entfernt werden? Welche Dokumente gilt es gesondert zu betrachten, weil es hierfür bereits definierte Workflows oder Zielsystem im Unternehmen gibt? Das Unternehmen profitiert von der Fileshareanalyse außerdem in folgenden Punkten: Mehr Sicherheit: Durch das Erkennen von fehlenden Rechtekonzepten können diese nachträglich ergänzt werden. Bessere Qualität der Daten: Durch das Aufdecken von Dubletten können diese bereinigt werden. Daten, die lange nicht mehr verwendet oder veraltet sind, können bequem in einem gesonderten Prozess gelöscht werden. Mehr Wissen: Dokumente werden mit Metadaten versehen, sodass der Nutzer weiß, welche Inhalte sich in den Daten befinden. Verborgenes Wissen kann so aufgedeckt und einem großen, berechtigten Nutzerkreis zur Verfügung gestellt werden. Weniger Kosten: Durch die Datenbereinigung lassen sich nicht zuletzt Speicherplatz und damit Kosten für weitere Server sparen. Bei einer Migration verringert sich gegebenenfalls das zu migrierende Datenvolumen aufgrund der gewonnen Erkenntnisse drastisch. Fileshareanalyse mit iFinder5 elastic und Kibana Sind alle zu analysierenden Datenquellen erfasst, werden sie im nächsten Schritt mit dem iFinder5 elastic indexiert und recherchierbar gemacht. Der iFinder5 elastic ist eine Enterprise Search Lösung für die unternehmensweite Suche in strukturierten und unstrukturierten Daten. Kibana ist eine Visualisierungsplattform von Elasticsearch, die die Suchoberfläche des iFinder um flexible Reportingfunktionen ergänzt. Darüber hinaus verfügt der iFinder5 elastic über zahlreiche Content Analytics Funktionen. Bereits beim Indexieren extrahiert der iFinder aus dem Dokumenten die Metadaten wie zum Beispiel Autor, Datum oder Titel und stellt diese später in der Trefferliste zur Ansicht bereit. Tauchen im Dokument Schlagworte, Produkte, Personen, Organisationen oder Orte auf, können diese außerdem mit dem Tagging Service ebenfalls automatisch erkannt werden. Diese Angaben werden dann als zusätzliche Metadaten im Index gespeichert und sind so für den Analysten recherchierbar.     Suchanfragen können einfach und schnell gefiltert werden. So lässt sich zum Beispiel bei einer Suche nach dem Begriff „confidential“ über die angebotenen Suchfilter Dateityp „PowerPoint“ und Autor „IntraFind“ sowie aus der Zeitleiste der „letzte Monat“ aus einem großen Datenbestand, eine überschaubare Menge an Inhalten filtern. Mit drei Klicks werden dadurch alle PowerPoint Präsentationen, in denen der Begriff „confidential“ vorkommt, des vergangenen Monats von IntraFind gefunden. In einer weiteren Ansicht der Analyseplattform lassen sich schnell die gewonnenen Metadaten anhand moderner, auf Kibana basierenden Dashboards selbständig in Reports zusammenfassen. Reports sind einfach und schnell neu definiert und wiederum für eine Gruppe von Analysten wiederverwertbar. So unterstützt der iFinder5 elastic die Fileshare-Analyse: Autovervollständigung: mit Tippfehlerkorrektur und „Meinten-Sie“-Vorschläge Preview: Vorschaufunktion für mehr als 600 Dateiformate und Hervorhebung der Trefferstelle in der Großpreview der iFinder-Benutzeroberfläche. Metadatenerzeugung: Über das automatische Verschlagworten als Standardfunktionalität des iFinder können fehlende Metadaten automatisch oder in einem qualitätsgesicherten Prozess aus den unstrukturierten Volltexten erzeugt und die Inhalte damit angereichert werden. Das System erzeugt folgende Metadaten: Top-Schlagworte, allgemeine Eigennamen von Personen, Unternehmen oder auch firmenspezifische Entitäten wie Produktnamen oder Abteilungsbezeichnungen sowie Themenzugehörigkeiten. Es können dann darauf aufbauend auch Relationen zwischen den Entitäten erkannt und angereichert werden. Dublettenprüfung: Filterung nach Dokumenten mit gleichem Inhalt beziehungsweise nach identischen Dokumenten Ähnliche Dokumente finden: Auf Basis der Schlagworte und Top-Terme eines gefundenen Dokuments werden im Bestand inhaltlich ähnliche Dokumente gefunden. Speicherung von kompletten Suchanfragen, Unterstützung von Kollaborations-Funktionalitäten Rollenkonzepte: der iFinder nutzt Rechte und Rollenkonzepte, mit denen sich unterschiedliche Ansichten auf die Inhalte der Dokumente steuern lassen. Ein Mitarbeiter aus der Rechtsabteilung könnte so beispielsweise auch unabhängig der Dokumentenrechte Inhalte der Dokumente sehen. Fachadministratoren könnte man nur Zugriff auf ihre „eigenen Daten“ gewähren und für IT Administratoren wäre es durchaus denkbar, dass sie die Metadaten aller Dokumente sehen, die das System erfasst. Der iFinder5 basiert auf der Technologie Elasticsearch und kennt keinerlei Limitierung bezüglich der zu erfassenden Datenmengen. Mehrere Milliarden Datensätze stellen für den iFinder keinerlei Hürde dar. Selbstverständlich ist eine inhaltliche Analyse der Daten auch in jedem anderem Quellsystem sinnvoll. Auch über Datenquellen hinweg ist die Analyse in vielen Anwendungsszenarien ein wichtiges Werkzeug dafür, um Sicherheit über die Dateninhalte und Datenstruktur zu gewinnen. Erfahren Sie hier mehr über die Content Analytics Suite for Artificial Intelligence. Fragen Sie uns nach erfolgreichen Projektbeispielen bei unseren Kunden!   Der Autor Manuel Brunner ist Experte für das Thema Suchtechnologien. Seit 2008 ist er für die IntraFind Software AG tätig und leitete u.a. viele Jahre lang das Team Professional Services, bevor er in seiner neuen Rolle die Bereiche Partner Management &amp; Business Development übernahm. Manuel Brunner is an expert in search technologies. He has been working at IntraFind Software AG since 2008 and has managed the team Professional Services for many years. In his new role he took over the divisions Partner Management &amp; Business Development.</body></Document><Document><url>https://www.intrafind.de/produkte/ifinder5-elastic/services_plugins</url><id>9A6E22FCD5FC08C05CD71F95645714E3</id><title>iFinder5 elastic Services &amp; Plugins</title><language>de</language><body>Services &amp; Plugins für Elasticsearch Die wachsenden Mengen vor allem unstrukturierter Daten stellen Unternehmen vor neue Herausforderungen im Umgang mit Dokumenten und Informationen. IntraFind erweitert mit seinen Services &amp; Plugins die führenden Open Source- Suchserver Elasticsearch und bietet Unternehmen Einfachheit und maximale Flexibilität für den Aufbau hochperformanter und skalierbarer Suchanwendungen, besonders im Big Data-Umfeld. Daraus ergeben sich wichtige Vorteile: Mit den IntraFind Services &amp; Plugins werden Elasticsearch schneller und einfacher nutzbar. Projektlaufzeiten und Kosten reduzieren sich durch die einfach und schnell integrierbaren IntraFind Services &amp; Plugins deutlich. Zudem ergänzt das Baukastenprinzip optimal den Funktionsumfang, welcher für die Umsetzung einer Suchanwendung erforderlich ist. Services und Plugins (z.B. für die Dokumentenkonvertierung, Treffervorschau, Autovervollständigung, Linguistik, Verschlagwortung mit Metadaten sowie für das Einbinden von externen Ressourcen wie Thesauri) sorgen für eine hohe Qualität der Suchergebnisse und damit für Benutzerfreundlichkeit. Die IntraFind Services und Plugins stehen als Cloud-Dienst (SaaS) oder als On-Premise-Lösung zur Verfügung. Zudem sind sie alleinstehend oder flexibel miteinander kombiniert einsetzbar. Folgende Eigenschaften zeichnen die IntraFind Services aus: Benutzer können die Services in geringer Zeit und mit geringen Vorkenntnissen implementieren und nutzen Alle Services sind schlank, auf Performance optimiert und einfach integrierbar Webschnittstelle im XML-, JSON-, Hessian- und SOAP-Format Verfügbarkeit von Load-Balancing und Ausfallsicherheit Minimale Installationsvoraussetzungen Gute Abstraktion der Suchtechnologie von der Implementierung Migration auf zukünftige Technologien ohne clientseitige Änderung möglich Beispielhafte Anordnung der IntraFind Services &amp; Plugins auf Basis von Elasticsearch IntraFind Services Search &amp; Index Service Extract Service Preview &amp; Extract Service Autocomplete Service Tagging Service Query Expansion Service SimFinder Service Summarizer Service IntraFind Plugins Linguistik Plugin</body></Document><Document><url>https://www.intrafind.de/produkte/elasticsearch/plugins_services/Preview_Service</url><id>17EB673685BEFCA479F77723DE535B0C</id><title>Elasticsearch Preview &amp; Extract Service</title><language>de</language><body>Preview &amp; Extract Service Der Preview &amp; Extract Service generiert aus über 600 Dateiformaten Vorschaubilder der Originaldokumente. Das erhaltene Bild kann im Anschluss in verschiedene Systeme (CMS, DMS, Trefferlisten etc.) eingebunden werden. Der Preview &amp; Extract Service ist ein optimaler Bestandteil jeder Suchlösung und unterstützt den Anwender im schnelleren Umgang mit Dokumenten.   Technische Spezifikation: Betriebssysteme Windows Server 2008 R2 oder höher sowie Linux Installationsvoraussetzungen Java 7 Kombinierbare Dienste: Der Preview &amp; Extract Service kann sowohl alleine als auch in Verbindung mit dem Search &amp; Index Service eingesetzt werden. Alle weiteren Services und Plugins können in einer Pipeline orthogonal kombiniert werden Dokumentation: SOA-Begleitdokumentation Beispielimplementierung in Java wird bereitgestellt Demo-Verzeichnis mit einfachen Click&amp;Play-Nutzungsbeispielen Für wen ist der Service interessant: Unternehmen, die eine visuelle Vorschau der Dokumente benötigen ohne die entsprechende Anwendung oder das Programm lokal installiert zu haben Embedding Partner Elasticsearch Entwickler Solr Entwickler DEMO-ZUGANG ANFORDERN &gt;&gt;</body></Document><Document><url>https://www.intrafind.de/blog/suche-im-pdm</url><id>B53E3EE7ED7FE99F6A2E673A5C7F25A7</id><title>5 Gründe für eine Suche in PDM Systemen</title><language>de</language><body>5 Gründe für eine Suche in PDM Systemen 5 Gründe, warum sich eine Suche im Produktdatenmanagement lohnt Produktdaten und produktrelevante Informationen entstehen in fast allen Bereichen eines Unternehmens und werden von fast allen Abteilungen benötigt. Die Werkstatt braucht Aussagen, welche Schrauben in einer Baugruppe verwendet werden dürfen, in der PR-Abteilung eines Autobauers ist eine Anfrage eines Kunden zum Thema gesundheitsgefährdende Stoffe in den verwendeten Kunststoffen oder im Lederbezug des Lenkrads zu beantworten. Oftmals fügen Mitarbeiter im Laufe des Entstehungszyklus eines Produkts weitere Daten hinzu. Der hohe Grad an Vernetzung der Produktdaten aufgrund steigender Produktkomplexität und die Forderung nach zuverlässiger Bereitstellung von Daten über den gesamten Lebenszyklus hinweg, stellen Verantwortlichen im Bereich Produktdatenmanagement vor große Herausforderungen. Nicht nur der Informationsbedarf nimmt zu, die Entwicklungszyklen werden immer kürzer und Unternehmen sind gezwungen, schnelle Antworten auf komplexe Fragestellungen zu finden. Wie kann sichergestellt werden, dass im Bedarfsfall die benötigten Daten vollständig und in adäquat aufbereiteter Form beim Adressaten landen? Gibt es überhaupt den EINEN Ort, an dem sich alle benötigten Informationen befinden? Ist sichergestellt, dass wirklich alle produktrelevanten Informationen dort zusammen kommen? Wer entscheidet, was produktrelevant ist? Das Marketing benötigt z.B. zum Thema Lenkrad andere Informationen als die Produktionsplanung. Oberste Priorität kommt der Bereitstellung von Normen, Versuchsberichten, Änderungsmitteilungen und den technischen Dokumentationen zu. Idealerweise sind mindestens diese Informationen in einem PDM System in aktueller Form vorhanden. Zusätzlich gibt es in jedem Unternehmen weitere, häufig sehr hilfreiche Information z.B. aus Gesprächsprotokollen, Informationen aus Krisensitzungen, Beschwerde- und Belobigungsschreiben, Serviceberichten oder Kommentaren zu Reparaturanleitungen. WIKIS Einträge, E-Mails oder Diskussionsforen sind ebenfalls durchaus hilfreiche und wichtige Informationsquellen, um die eine oder andere Fragestellung schneller und umfassend beantworten zu können. Einbindung aller relevanten Datenquellen PDM Systeme lassen eine wesentliche Funktion vermissen: Es existiert meist kein Ranking. Bei der Suche muss der User unter Umständen große Listen durchsuchen und jedes Ergebnis einzeln prüfen. Hierdurch wird wertvolle Arbeitszeit verschwendet. Manche Systeme bieten überhaupt keine intelligenten Filter und Selektionsmechanismen an, was die Suche nicht gerade erleichtert. Selten kann man sich darauf verlassen, dass angezeigte Informationen wirklich vollständig und aktuell sind, die Anwender können sich eigentlich nie so 100-prozentig sicher sein, ob sie auch wirklich alle relevanten Informationen gefunden haben. Das Ganze ist dann noch dazu zeitraubend und zeigt auf, dass Informationsgewinnung ein echter Kostenfaktor ist. In der Praxis ist nicht automatisch gewährleistet, dass wichtige Informationen, die die Produkte betreffen, automatisch auch im PDM System landen. Oft sind z.B. die Prozesse für die Zulieferung überhaupt nicht genau definiert und man läuft als PDM Verantwortlicher sozusagen den Daten hinterher.   Werden dann tatsächlich mal schnell irgendwelche Informationen benötigt, geht die Suche erst richtig los. Müssen dann noch andere Datenquellen mit unterschiedlichen Abfragemechanismen in die Suche mit einbezogen werden, wird es schwierig.  Wie bekommt man nun die Informationen aus den unterschiedlichen Quellen zusammen, natürlich unter Einhaltung der individuell geregelten, manchmal sehr komplexen Zugriffsberechtigungen? Die Lösung liegt im Aufsetzen einer übergreifenden Suche, an die sowohl das PDM System, als auch die diversen anderen Datenquellen angeschlossen werden. Dadurch wird sichergestellt, dass die Anwender auf Informationen aus verschiedenen Datenquellen zugreifen können, egal ob z.B. Intranet, Filesystem oder einer  Bibliotheksapplikation. Einfache Bedienung auch für fachfremde Kollegen Ein weiterer Vorteil: Mitarbeitern, die nicht den ganzen Tag mit PDM Systemen zu tun haben, kann man über eine intuitiv bedienbare Oberfläche mit durchgängiger, einheitlicher Bediensystematik mehr Suchkomfort bieten. Das komplizierte Bedienen eines PDM Systems wird somit umgangen, Berührungsängste gehen verloren, die Zahl der User und die Nutzung des Systems steigen sprunghaft an. Auch die Einarbeitung neuer Kollegen kann dadurch  deutlich vereinfacht werden. Nicht zuletzt lassen sich durch die Integration der Suche in das PDM System neue Nutzerkreise erschließen, die nicht mehr auf Spezialisten angewiesen sind, sondern durchaus selbst in der Lage sind, Recherchen durchzuführen. Natürlich lässt sich so ein Suchsystem auch genau andersrum nutzen, nämlich zum Auffinden von Spezialisten. In der Praxis zeigt sich, dass die PDM Systeme durch die Hinzunahme einer komfortablen Suche eine ganz neue Akzeptanz erfahren. Das liegt daran, dass das System nun einfacher zu bedienen ist.   Vollständige Trefferliste nach Relevanzkriterien Nutzer erhalten eine übersichtliche Trefferliste und können davon ausgehen, dass diese auch vollständig ist und sie können über diese angezeigten Treffer direkt auf ihre gefundenen Dokumente und Informationen zugreifen. Wichtig ist dabei natürlich die Einhaltung der individuellen Zugriffsrechte. Das funktioniert übrigens auch mit der Autovervollständigung bei der Eingabe eins Suchworts. Auch das ist rechtegeprüft! Die Trefferliste wird nach Relevanzkriterien gebildet, die über ein PDM System überhaupt nicht zu Verfügung gestellt werden könnte. Intelligente Filtermechanismen als Facetten und Suchfilter erleichtern dem User nochmals das weitere schnelle Eingrenzen. Die Suche funktioniert natürlich auch über mehrsprachigen Content hinweg.   Linguistische Funktionalitäten wie Kompositazerlegung Das Herzstück einer guten Suchmaschine ist die linguistische Aufbereitung des Index. Dadurch lassen sich qualitativ hochwertige und vollständige Suchergebnisse erzielen. Suchen Mitarbeiter beispielsweise nach der Beschreibung einer Ölablassschraube, wollen sie  vielleicht auch die Dokumente schnell finden, die Informationen über die „Abdichtung an der Ölwanne mit einer Schraube“ enthalten. Zusätzliche Mehrwerte ergeben sich durch die Einbeziehung von branchen- und firmenspezifischen Ausdrücken und Synonymen. Man kann also mit einem Klick Suchanfragen erweitern, Vorschläge dafür liefert zum Beispiel ein firmen- oder branchenspezifischer Thesaurus - oder im einfachsten Fall eine selbst erstellte Liste mit Akronymen über häufige firmeninterne Abkürzungen.   Mehr Ergebnisse dank semantischer Suche Die semantische Suche schlägt weitere Suchterme vor, die in engem Zusammenhang mit ihrem eigentlichen Suchwort stehen. Dadurch lässt sich ebenfalls die Trefferliste schnell minimieren. Die gefundenen Dokumente lassen sich als Favoriten kennzeichnen und in einem separaten Bereich speichern.   Eine Wissenslandkarte ermöglicht zudem eine Suche, ohne dafür Suchwort eingeben zu müssen. Hier werden Metadaten dynamisch gefiltert und die potentielle Treffermenge eingegrenzt.   Fazit Durch eine professionelle Enterprise Search Lösung kann die Suche in PDM-Systemen mit einfachen Mitteln verbessert und effizienter gestaltet werden. Informations- und Wissensschätze bleiben dadurch nicht in irgendwelchen Ordnern oder Laufwerken verborgen, sondern können gezielt gefunden und genutzt werden. Dank der vielfältigen Anbindungsmöglichkeiten an unterschiedliche Systeme findet der Mitarbeiter auch die Daten, die nicht direkt im PDM-System hinterlegt sind – seien es Einträge aus Wikis, Mails oder anderen Systemen. Die Autocomplete-Funktion macht es möglich, dass der Nutzer auch dann die richtigen Begriffe findet, wenn er sich vertippt. Die integrierte Rechteverwaltung gewährleistet zudem, dass Abteilungen nur auf die Dokumente zugreifen können, die sie berechtigt sind zu sehen.   Der Autor Nach dem BWL Studium war Rutger Lörch in verschiedenen Positionen bei namhaften Hardware- und Softwareanbietern wie Nixdorf, Digital und Oracle tätig. Seit 2008 unterstützt er beim Suchspezialisten IntraFind den Vertrieb. „Das Thema Enterprise Search ist deshalb so faszinierend, weil die Anforderungen der jeweiligen Interessenten sehr individuell sind. Ein sehr abwechslungsreiches Betätigungsfeld voller spannender Herausforderungen.“</body></Document><Document><url>https://www.intrafind.de/news</url><id>43BCC0F0832DBB6B8AB1AF544F29047E</id><title>Neuigkeiten der IntraFind Software AG</title><language>de</language><body>IntraFind News Auf dieser Seite finden Sie aktuelle Neuigkeiten der IntraFind Software AG. Presse-Material stellen wir Ihnen auf Anfrage gerne zur Verfügung.</body></Document><Document><url>https://www.intrafind.de/unternehmen/news/google-verabschiedet-sich-laut-us-portal-von-google-search-appliance</url><id>9922D9886255BF2FBB367E4ADE965345</id><title>09/02/2016 - GOOGLE verabschiedet sich laut US-Portal von Google Search Appliance</title><language>de</language><body>GOOGLE verabschiedet sich laut US-Portal von Google Search Appliance München, 09.02.2016 – Fortune.com veröffentlichte kürzlich, dass Google den Verkauf und den Support des Produktes Google Search Appliance (GSA) bis 2018 einstellen wird. Das Portal beruft sich dabei auf verschiedene Quellen von Google-Partnern. Google selbst äußerte sich bisher nicht zu dieser Information. Das seit 2002 auf dem Markt befindliche und vom Analystenhaus Gartner als Challenger eingestufte Produkt wird aktuell auch von zahlreichen deutschen Unternehmen eingesetzt. Kunden und Partner der GSA - Zeit für Veränderung IntraFind, einer der führenden europäischen Softwarehersteller und seit dem Jahr 2000 mit dem Enterprise Search Produkt iFinder auf dem Markt, bietet für Interessenten, Kunden und Partner der Google Search Appliance eine interessante und nahtlos integrierbare Softwarelösung sowie kompetente Beratung für mögliche Migrations- oder Ablöseprojekte. Lernen Sie unser Produkt iFinder5 elastic kennen. Sie nutzen in Ihrem Unternehmen derzeit die Google Search Appliance (GSA) und haben Interesse an einer Alternative? Kontaktieren Sie uns. Sie sind Google Partner und auf der Suche nach einer neuen Enterprise Search-Lösung für Ihr Portfolio? Wir freuen uns auf Ihre Nachricht. Über die IntraFind Software AG IntraFind entwickelt Produkte und Lösungen für das effiziente Suchen, Finden, Analysieren von Informationen unter Berücksichtigung aller Datenquellen eines Unternehmens. Volltextsuche und die komplette Bandbreite an Textanalyseverfahren bilden die Grundlage für optimale Rechercheergebnisse. Das IntraFind-Lösungsspektrum reicht von der Suche in einer Applikation, Enterprise Search und Metadatenmanagement bis hin zu spezialisierten, suchbasierten Anwendungen und Textanalyselösungen. IntraFind betreut seine Kunden umfassend – beginnend mit einer Bedarfsanalyse bietet IntraFind Beratung, Konzeption und Umsetzung sowie Unterstützung im laufenden Betrieb an. Die IntraFind Software AG wurde im Jahr 2000 gegründet und hat ihren Firmensitz in München. Namhafte Kunden von IntraFind sind: AUDI AG, Robert Bosch GmbH, MAN Truck &amp; Bus AG, Voith GmbH, ZEIT ONLINE GmbH. Für weitere Informationen wenden Sie sich bitte an IntraFind Software AG Sonja Bellaire Landsberger Straße 368 D-80687 München Email: sonja.bellaire@intrafind.de Internet: https://www.intrafind.de</body></Document><Document><url>https://www.intrafind.de/unternehmen/news/scherdel-nutzt-ifinder5-elastic-fuer-den-digitalen-arbeitsplatz</url><id>21FED68026C467959723A57049BF44C4</id><title>21/07/2017 - Scherdel nutzt iFinder5 elastic für den Digitalen Arbeitsplatz</title><language>de</language><body>Scherdel nutzt iFinder5 elastic für den Digitalen Arbeitsplatz München, den 21. Juli 2017 – Die Martkredwitzer Scherdel Gruppe, ein international agierender Hersteller und Lieferant von Metallprodukten und Lösungen für den Anlagenbau verschiedenster Branchen, setzt für den neuen „Digitalen Arbeitsplatz“ auf Suchtechnologien der IntraFind.  Im ersten Schritt sollen Mitarbeiter die für sie wichtigen Reports und Berichte innerhalb eines Portals noch schneller und effizienter finden. Für die Suche setzt Scherdel den iFinder5 elastic ein. Mittelfristig werden weitere Anwendungsbereiche erschlossen. Die Suchfunktionen werden hier von einem weiteren Partner der Scherdel Gruppe voll in das neu entstehende Portal integriert. Vor allem die zahlreichen Linguistik-Funktionen im iFinder5 elastic waren ausschlaggebend dafür, dass sich Scherdel für eine Suchlösung von IntraFind entschieden hat. Dank intelligenter Autokorrektur und Kompositazerlegung muss z.B. der Mitarbeiter aus dem Controlling den exakten Namen des jeweiligen Reports nicht zwingend kennen. Der entsprechende Begriff wird automatisch erkannt und vorgeschlagen. Der iFinder5 elastic ist somit für die Scherdel Gruppe einen wesentliche Komponente des neuen Digitalen Arbeitsplatzes.   Über IntraFind Software AG IntraFind entwickelt Produkte und Lösungen für das effiziente Suchen, Finden, Analysieren von strukturierten und unstrukturierten Informationen unter Berücksichtigung aller verfügbaren Datenquellen eines Unternehmens. Volltextsuche und die komplette Bandbreite an Textanalyse- und Machine Learning-Verfahren, Natural Language Processing, kombiniert mit den Möglichkeiten von Graphdatenbanken für Big Data Analytics, bilden hierbei den Schwerpunkt. Namhafte Kunden sind: AUDI AG, BMW AG, Bundeswehr, IHK Berlin, Robert Bosch GmbH und Rohde &amp; Schwarz GmbH &amp; Co. KG. Mehr Informationen: www.intrafind.de. Für weitere Informationen wenden Sie sich bitte an   IntraFind Software AG Christiane Stagge Landsberger Straße 368 D-80687 München Email: christiane.stagge@intrafind.de Internet: https://www.intrafind.de  </body></Document></documents>